{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sst_classifier.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1lqbTTNPpFB",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Classification with a Deep Learning Model\n",
        "\n",
        "This notebook introduces a machine learning task from the field of natural language processing (machine learning focused on the processing of spoken and written text).\n",
        "\n",
        "## Sentiment Analysis\n",
        "\n",
        "The modelled task is a classification task called sentiment analysis. \n",
        "Text snippets are classified according to their positive or negative sentiment that is expressed in them. \n",
        "This can be modelled as 3-class problem (negative, neutral, positive), or as a degree of sentiment on a 5-class or 10-class scale. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Acknowledgement\n",
        "\n",
        "The notebook is based on https://www.manning.com/books/real-world-natural-language-processing, an upcoming book focused on NLP.\n",
        "\n",
        "The ML frameworks used are:\n",
        "\n",
        "* pytorch\n",
        "* allennlp\n",
        "* spacy\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/erikgraf/deepLearning/blob/master/Deep_Learning_Sentiment_classifier.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CbtvBLYPpFJ",
        "colab_type": "text"
      },
      "source": [
        "## Installing Dependencies\n",
        "\n",
        "The cell below installs the main dependencies and clones some a repository that forms the basis of the implementation. \n",
        "\n",
        "Executing it with `CTRL + Enter` (`STRG +Enter` on a german keyboard) could take a couple of minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2WUYWOVTcajS",
        "outputId": "9adb28ab-d12d-493a-c04e-b8b7fc779e46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install allennlp\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.9)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.3)\n",
            "Collecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 50.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 53.4MB/s \n",
            "\u001b[?25hCollecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/a6/e69e38f1f259fcf8532d8bd2c4bc88764f42d7b35a41423a7f4b035cc5ce/jsonnet-0.14.0.tar.gz (253kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 52.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n",
            "\u001b[?25hCollecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 49.3MB/s \n",
            "\u001b[?25hCollecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/72/dd/ac49f9c69540d7e09210415801a05d0a54d4d0ca8401503c46847dacd3a0/overrides-2.8.0.tar.gz\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.3)\n",
            "Collecting numpydoc>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/70/4d8c3f9f6783a57ac9cc7a076e5610c0cc4a96af543cafc9247ac307fbfe/numpydoc-0.9.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.17.4)\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.10.40)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 49.2MB/s \n",
            "\u001b[?25hCollecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/3e/0c/940781dd49710f4b1f0650c450c9fd8491db0e1bffd99ebc36355607f96d/responses-0.10.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->allennlp) (1.12.0)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (2019.12.9)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 48.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (42.0.2)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.0.2)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.16.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (2.10.3)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.13.40)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp) (1.1.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.2)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.15.2)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
            "Building wheels for collected packages: jsonnet, parsimonious, ftfy, overrides, numpydoc, word2number\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.14.0-cp36-cp36m-linux_x86_64.whl size=3320382 sha256=371e2085580539fdbf7e8bca9a15d0b3de4ebe5f477bdbc0fc5f74d535f9b090\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/b7/83/985f0f758fbb34f14989a0fab86d18890d1cc5ae12f26967bc\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=78622d23beee25815165486a8f52893e9bb168d9032f4e8b1e2586b0e138a0e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.6-cp36-none-any.whl size=44553 sha256=c51a3b23f017e3bfd3800bd1a4a41eb84c4ed928d8f6260d0595d3d197889030\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/34/ce/cbb38d71543c408de56f3c5e26ce8ba495a0fa5a28eaaf1046\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.8.0-cp36-none-any.whl size=5608 sha256=e6825c492d56a7ab14c598c41cc5234428ccb10aaf8811c149022e54ba73a8fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/f1/ba/eaf6cd7d284d2f257dc71436ce72d25fd3be5a5813a37794ab\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpydoc: filename=numpydoc-0.9.2-cp36-none-any.whl size=31894 sha256=713eada72884c54c263a1440c3806cf7111f8f4760c49b8387375721c0f8ace2\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/f3/52/25c8e1f40637661d27feebc61dae16b84c7cdd93b8bc3d7486\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=0dd54d940ff06e62766408ff3569c6b347d79176f3a42b632b964a7a37ea33f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "Successfully built jsonnet parsimonious ftfy overrides numpydoc word2number\n",
            "Installing collected packages: sentencepiece, pytorch-transformers, tensorboardX, flask-cors, conllu, jsonnet, parsimonious, ftfy, pytorch-pretrained-bert, overrides, numpydoc, flaky, word2number, jsonpickle, unidecode, responses, allennlp\n",
            "Successfully installed allennlp-0.9.0 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.6 jsonnet-0.14.0 jsonpickle-1.2 numpydoc-0.9.2 overrides-2.8.0 parsimonious-0.8.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.9 sentencepiece-0.1.85 tensorboardX-2.0 unidecode-1.1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni4FWBoJPpFl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "23f3e148-a55d-4668-faf7-378d346880ab"
      },
      "source": [
        "!git clone https://github.com/mhagiwara/realworldnlp.git\n",
        "%cd realworldnlp"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'realworldnlp'...\n",
            "remote: Enumerating objects: 168, done.\u001b[K\n",
            "remote: Counting objects: 100% (168/168), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 456 (delta 102), reused 94 (delta 41), pack-reused 288\u001b[K\n",
            "Receiving objects: 100% (456/456), 4.69 MiB | 19.07 MiB/s, done.\n",
            "Resolving deltas: 100% (231/231), done.\n",
            "/content/realworldnlp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIoxbXqOPpFx",
        "colab_type": "text"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Execute the cell below to load all required modules. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FM5pBpj7cajc",
        "colab": {}
      },
      "source": [
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from allennlp.data.dataset_readers.stanford_sentiment_tree_bank import \\\n",
        "    StanfordSentimentTreeBankDatasetReader\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.nn.util import get_text_field_mask\n",
        "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
        "from allennlp.training.trainer import Trainer\n",
        "\n",
        "from realworldnlp.predictors import SentenceClassifierPredictor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LmBTaVlPpGC",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "The cell below sets the hyperparameters.\n",
        "\n",
        "* EMBEDDING_DIM: This is the dimensionality of the word embeddings (numeric representations of words such as word2vec or glove (https://nlp.stanford.edu/projects/glove/))\n",
        "* HIDDEN_DIM: This is the dimensionality of the LSTM (Long Short Term Memory) Deep Learning network. \n",
        "\n",
        "A value of 128 is pretty standard for the embeddings and hidden_dim.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jecKS_nhcajq",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzH3khPHPpGN",
        "colab_type": "text"
      },
      "source": [
        "## Training Data Set\n",
        "\n",
        "For training we will use the Stanford Sentiment Treebank data set.\n",
        "A data set for training sentiment analysis models. It is annotated both on the sentence and the word level with regard to the sentiment. \n",
        "\n",
        "When loading the data set we can configure the granularity to `'5-class'` or `'3-class'`.\n",
        "\n",
        "`'3-class'` represents classification on the level of `negative`, `neutral`, `positive` encoded as `0`, `1`, `2` (positive). `'5-class'` on a level from `0` to `4`.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b-yyKUYxcaj1",
        "colab": {}
      },
      "source": [
        "reader = StanfordSentimentTreeBankDatasetReader(granularity='5-class')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N4PBhQZAcaj6",
        "outputId": "31b015e8-01f7-4b93-c12b-384b08f6c8bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "train_dataset = reader.read('https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/train.txt')\n",
        "dev_dataset = reader.read('https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/dev.txt')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n",
            "  0%|          | 0/2160058 [00:00<?, ?B/s]\u001b[A\n",
            " 94%|█████████▍| 2029568/2160058 [00:00<00:00, 19946073.59B/s]\u001b[A\n",
            "8544it [00:02, 3972.52it/s]\n",
            "0it [00:00, ?it/s]\n",
            "  0%|          | 0/280825 [00:00<?, ?B/s]\u001b[A\n",
            "1101it [00:00, 2845.24it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1c2Rh7tPpGg",
        "colab_type": "text"
      },
      "source": [
        "## Model Implementation in AllenNLP\n",
        "\n",
        "Execute the cell below to load the model classification.\n",
        "\n",
        "Depending on the class level chosen (3 vs 5) change the positive label in the init method to ('2' or '4').\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5V2kzVAlcajw",
        "colab": {}
      },
      "source": [
        "# Model in AllenNLP represents a model that is trained.\n",
        "@Model.register(\"lstm_classifier\")\n",
        "class LstmClassifier(Model):\n",
        "    def __init__(self,\n",
        "                 word_embeddings: TextFieldEmbedder,\n",
        "                 encoder: Seq2VecEncoder,\n",
        "                 vocab: Vocabulary,\n",
        "                 positive_label: str = '4') -> None:\n",
        "        super().__init__(vocab)\n",
        "        # We need the embeddings to convert word IDs to their vector representations\n",
        "        self.word_embeddings = word_embeddings\n",
        "\n",
        "        self.encoder = encoder\n",
        "\n",
        "        # After converting a sequence of vectors to a single vector, we feed it into\n",
        "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
        "        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
        "                                      out_features=vocab.get_vocab_size('labels'))\n",
        "\n",
        "        # Monitor the metrics - we use accuracy, as well as prec, rec, f1 for 4 (very positive)\n",
        "        positive_index = vocab.get_token_index(positive_label, namespace='labels')\n",
        "        self.accuracy = CategoricalAccuracy()\n",
        "        self.f1_measure = F1Measure(positive_index)\n",
        "\n",
        "        # We use the cross entropy loss because this is a classification task.\n",
        "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
        "        # which makes it unnecessary to add a separate softmax layer.\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Instances are fed to forward after batching.\n",
        "    # Fields are passed through arguments with the same name.\n",
        "    def forward(self,\n",
        "                tokens: Dict[str, torch.Tensor],\n",
        "                label: torch.Tensor = None) -> torch.Tensor:\n",
        "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
        "        # shorter sequences get padded with zeros to make them equal length.\n",
        "        # Masking is the process to ignore extra zeros added by padding\n",
        "        mask = get_text_field_mask(tokens)\n",
        "\n",
        "        # Forward pass\n",
        "        embeddings = self.word_embeddings(tokens)\n",
        "        encoder_out = self.encoder(embeddings, mask)\n",
        "        logits = self.linear(encoder_out)\n",
        "\n",
        "        # In AllenNLP, the output of forward() is a dictionary.\n",
        "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
        "        output = {\"logits\": logits}\n",
        "        if label is not None:\n",
        "            self.accuracy(logits, label)\n",
        "            self.f1_measure(logits, label)\n",
        "            output[\"loss\"] = self.loss_function(logits, label)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        precision, recall, f1_measure = self.f1_measure.get_metric(reset)\n",
        "        return {'accuracy': self.accuracy.get_metric(reset),\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1_measure': f1_measure}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89KdsW5kPpGq",
        "colab_type": "text"
      },
      "source": [
        "## Transform Text into Numeric Representation\n",
        "\n",
        "The following cells are responsible for the transformation of text in string form into numeric representations that are suitable as learning input for the neural network.\n",
        "\n",
        "1. Extract vocabulary of unique terms from the text\n",
        "2. Create embeddings for the terms\n",
        "3. Define transformation (encoding) for a sequence of text (i.e. a sentence)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OuchQwj5cakA",
        "outputId": "4813cf15-20b1-465f-caef-591dd4cf0ead",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# You can optionally specify the minimum count of tokens/labels.\n",
        "# `min_count={'tokens':3}` here means that any tokens that appear less than three times\n",
        "# will be ignored and not included in the vocabulary.\n",
        "vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
        "                                  min_count={'tokens': 3})"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9645/9645 [00:00<00:00, 57237.94it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kDx14NvHcakC",
        "colab": {}
      },
      "source": [
        "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                            embedding_dim=EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VKXSf0wucakG",
        "colab": {}
      },
      "source": [
        "# BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,\n",
        "# not for labels, which are used as-is as the \"answer\" of the sentence classification\n",
        "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "93pvAweOcakM",
        "colab": {}
      },
      "source": [
        "# Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
        "# (usually a sequence of embedded word vectors), processes it, and returns a single\n",
        "# vector. Oftentimes this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
        "# AllenNLP also supports CNNs and other simple architectures (for example,\n",
        "# just averaging over the input vectors).\n",
        "encoder = PytorchSeq2VecWrapper(\n",
        "    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4atXjVUsPpHI",
        "colab_type": "text"
      },
      "source": [
        "## Configure Model for Training\n",
        "\n",
        "The following four cells configure the model for training.\n",
        "\n",
        "1. The LstmClassifier class takes the word_embeddings, the define sequence encoder and the vocabulary as input configuration. \n",
        "\n",
        "2. The BucketIterator is a helper class for iterating over the full training set and randomly selects batches of instances for the training. \n",
        "\n",
        "3. optimizer specifies the learning rate for Adam (a mathmatical optimisation function that will guide the weight adaptations of our model).\n",
        "\n",
        "4. trainer holds our instatiation of the model, and defines the number of epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZuuP66iccakR",
        "colab": {}
      },
      "source": [
        "model = LstmClassifier(word_embeddings, encoder, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ip0BO9QecakY",
        "colab": {}
      },
      "source": [
        "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
        "iterator.index_with(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ccuqvd6rcakg",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uu_dBwd1cakk",
        "colab": {}
      },
      "source": [
        "trainer = Trainer(model=model,\n",
        "                  optimizer=optimizer,\n",
        "                  iterator=iterator,\n",
        "                  train_dataset=train_dataset,\n",
        "                  validation_dataset=dev_dataset,\n",
        "                  patience=40,\n",
        "                  num_epochs=40)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDt-Kg1oPpHo",
        "colab_type": "text"
      },
      "source": [
        "## Train\n",
        "\n",
        "Execute the cell below to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55xc9RfYPpHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bed5b7d8-dc1c-490c-b2f9-659fdf53c52a"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.2536, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5813 ||: 100%|██████████| 267/267 [00:14<00:00, 16.59it/s]\n",
            "accuracy: 0.2625, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5743 ||: 100%|██████████| 35/35 [00:00<00:00, 79.39it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.2802, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5659 ||: 100%|██████████| 267/267 [00:14<00:00, 18.08it/s]\n",
            "accuracy: 0.2598, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5728 ||: 100%|██████████| 35/35 [00:00<00:00, 94.80it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.2786, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5572 ||: 100%|██████████| 267/267 [00:14<00:00, 18.80it/s]\n",
            "accuracy: 0.2625, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5680 ||: 100%|██████████| 35/35 [00:00<00:00, 96.11it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.2996, precision: 0.6000, recall: 0.0256, f1_measure: 0.0491, loss: 1.5266 ||: 100%|██████████| 267/267 [00:14<00:00, 18.53it/s]\n",
            "accuracy: 0.2916, precision: 0.3659, recall: 0.0909, f1_measure: 0.1456, loss: 1.5543 ||: 100%|██████████| 35/35 [00:00<00:00, 93.06it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.3619, precision: 0.4145, recall: 0.3727, f1_measure: 0.3925, loss: 1.4560 ||: 100%|██████████| 267/267 [00:14<00:00, 18.66it/s]\n",
            "accuracy: 0.3088, precision: 0.2360, recall: 0.4848, f1_measure: 0.3175, loss: 1.5290 ||: 100%|██████████| 35/35 [00:00<00:00, 97.95it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.4364, precision: 0.4206, recall: 0.6685, f1_measure: 0.5163, loss: 1.3297 ||: 100%|██████████| 267/267 [00:13<00:00, 19.55it/s]\n",
            "accuracy: 0.3588, precision: 0.2857, recall: 0.5091, f1_measure: 0.3660, loss: 1.4938 ||: 100%|██████████| 35/35 [00:00<00:00, 97.85it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.5085, precision: 0.5130, recall: 0.7345, f1_measure: 0.6041, loss: 1.2032 ||: 100%|██████████| 267/267 [00:13<00:00, 19.19it/s]\n",
            "accuracy: 0.3715, precision: 0.3674, recall: 0.4788, f1_measure: 0.4158, loss: 1.4734 ||: 100%|██████████| 35/35 [00:00<00:00, 97.85it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.5715, precision: 0.6226, recall: 0.7686, f1_measure: 0.6880, loss: 1.0717 ||: 100%|██████████| 267/267 [00:14<00:00, 23.55it/s]\n",
            "accuracy: 0.3815, precision: 0.3910, recall: 0.3697, f1_measure: 0.3801, loss: 1.5815 ||: 100%|██████████| 35/35 [00:00<00:00, 95.72it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.6169, precision: 0.7052, recall: 0.7911, f1_measure: 0.7457, loss: 0.9671 ||: 100%|██████████| 267/267 [00:13<00:00, 19.08it/s]\n",
            "accuracy: 0.3688, precision: 0.3678, recall: 0.3879, f1_measure: 0.3776, loss: 1.6263 ||: 100%|██████████| 35/35 [00:00<00:00, 94.82it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.6506, precision: 0.7539, recall: 0.8183, f1_measure: 0.7848, loss: 0.8831 ||: 100%|██████████| 267/267 [00:13<00:00, 22.50it/s]\n",
            "accuracy: 0.3688, precision: 0.4080, recall: 0.4303, f1_measure: 0.4189, loss: 1.6237 ||: 100%|██████████| 35/35 [00:00<00:00, 97.02it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.6855, precision: 0.7860, recall: 0.8439, f1_measure: 0.8139, loss: 0.8065 ||: 100%|██████████| 267/267 [00:13<00:00, 19.23it/s]\n",
            "accuracy: 0.3615, precision: 0.3871, recall: 0.3636, f1_measure: 0.3750, loss: 1.7608 ||: 100%|██████████| 35/35 [00:00<00:00, 92.23it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.7110, precision: 0.8023, recall: 0.8571, f1_measure: 0.8288, loss: 0.7468 ||: 100%|██████████| 267/267 [00:14<00:00, 18.34it/s]\n",
            "accuracy: 0.3669, precision: 0.3684, recall: 0.3818, f1_measure: 0.3750, loss: 1.8179 ||: 100%|██████████| 35/35 [00:00<00:00, 96.95it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.7367, precision: 0.8310, recall: 0.8781, f1_measure: 0.8539, loss: 0.6857 ||: 100%|██████████| 267/267 [00:13<00:00, 18.80it/s]\n",
            "accuracy: 0.3569, precision: 0.3616, recall: 0.3879, f1_measure: 0.3743, loss: 1.9775 ||: 100%|██████████| 35/35 [00:00<00:00, 93.96it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.7587, precision: 0.8320, recall: 0.8843, f1_measure: 0.8574, loss: 0.6422 ||: 100%|██████████| 267/267 [00:14<00:00, 19.01it/s]\n",
            "accuracy: 0.3560, precision: 0.3871, recall: 0.3636, f1_measure: 0.3750, loss: 1.9588 ||: 100%|██████████| 35/35 [00:00<00:00, 98.33it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.7772, precision: 0.8540, recall: 0.9037, f1_measure: 0.8782, loss: 0.6046 ||: 100%|██████████| 267/267 [00:14<00:00, 19.05it/s]\n",
            "accuracy: 0.3615, precision: 0.3605, recall: 0.3758, f1_measure: 0.3680, loss: 2.1313 ||: 100%|██████████| 35/35 [00:00<00:00, 94.20it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.7924, precision: 0.8580, recall: 0.9099, f1_measure: 0.8832, loss: 0.5645 ||: 100%|██████████| 267/267 [00:13<00:00, 19.16it/s]\n",
            "accuracy: 0.3642, precision: 0.3593, recall: 0.3636, f1_measure: 0.3614, loss: 2.2841 ||: 100%|██████████| 35/35 [00:00<00:00, 95.27it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.8048, precision: 0.8578, recall: 0.9037, f1_measure: 0.8802, loss: 0.5353 ||: 100%|██████████| 267/267 [00:14<00:00, 18.93it/s]\n",
            "accuracy: 0.3669, precision: 0.3803, recall: 0.3273, f1_measure: 0.3518, loss: 2.3701 ||: 100%|██████████| 35/35 [00:00<00:00, 96.78it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.8232, precision: 0.8768, recall: 0.9115, f1_measure: 0.8938, loss: 0.5052 ||: 100%|██████████| 267/267 [00:13<00:00, 19.10it/s]\n",
            "accuracy: 0.3488, precision: 0.3468, recall: 0.3636, f1_measure: 0.3550, loss: 2.5797 ||: 100%|██████████| 35/35 [00:00<00:00, 98.92it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.8278, precision: 0.8865, recall: 0.9216, f1_measure: 0.9037, loss: 0.4793 ||: 100%|██████████| 267/267 [00:14<00:00, 18.96it/s]\n",
            "accuracy: 0.3506, precision: 0.3459, recall: 0.3879, f1_measure: 0.3657, loss: 2.5370 ||: 100%|██████████| 35/35 [00:00<00:00, 96.24it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.8435, precision: 0.8872, recall: 0.9278, f1_measure: 0.9070, loss: 0.4455 ||: 100%|██████████| 267/267 [00:14<00:00, 18.95it/s]\n",
            "accuracy: 0.3642, precision: 0.3653, recall: 0.3697, f1_measure: 0.3675, loss: 2.6561 ||: 100%|██████████| 35/35 [00:00<00:00, 97.84it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.8480, precision: 0.9000, recall: 0.9293, f1_measure: 0.9144, loss: 0.4344 ||: 100%|██████████| 267/267 [00:14<00:00, 18.21it/s]\n",
            "accuracy: 0.3642, precision: 0.3636, recall: 0.3394, f1_measure: 0.3511, loss: 2.8954 ||: 100%|██████████| 35/35 [00:00<00:00, 95.27it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.8399, precision: 0.8803, recall: 0.9193, f1_measure: 0.8994, loss: 0.4543 ||: 100%|██████████| 267/267 [00:13<00:00, 17.76it/s]\n",
            "accuracy: 0.3660, precision: 0.3584, recall: 0.3758, f1_measure: 0.3669, loss: 2.7868 ||: 100%|██████████| 35/35 [00:00<00:00, 97.00it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.8612, precision: 0.9174, recall: 0.9394, f1_measure: 0.9283, loss: 0.4018 ||: 100%|██████████| 267/267 [00:14<00:00, 18.48it/s]\n",
            "accuracy: 0.3560, precision: 0.3734, recall: 0.3576, f1_measure: 0.3653, loss: 2.8830 ||: 100%|██████████| 35/35 [00:00<00:00, 96.16it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.8752, precision: 0.9189, recall: 0.9410, f1_measure: 0.9298, loss: 0.3718 ||: 100%|██████████| 267/267 [00:13<00:00, 19.19it/s]\n",
            "accuracy: 0.3569, precision: 0.3559, recall: 0.3818, f1_measure: 0.3684, loss: 2.9820 ||: 100%|██████████| 35/35 [00:00<00:00, 100.22it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.8826, precision: 0.9157, recall: 0.9441, f1_measure: 0.9297, loss: 0.3553 ||: 100%|██████████| 267/267 [00:14<00:00, 19.07it/s]\n",
            "accuracy: 0.3560, precision: 0.3636, recall: 0.3636, f1_measure: 0.3636, loss: 3.0740 ||: 100%|██████████| 35/35 [00:00<00:00, 97.57it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.8878, precision: 0.9206, recall: 0.9457, f1_measure: 0.9330, loss: 0.3459 ||: 100%|██████████| 267/267 [00:13<00:00, 19.32it/s]\n",
            "accuracy: 0.3633, precision: 0.3775, recall: 0.3455, f1_measure: 0.3608, loss: 3.1891 ||: 100%|██████████| 35/35 [00:00<00:00, 95.23it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.8901, precision: 0.9179, recall: 0.9457, f1_measure: 0.9315, loss: 0.3406 ||: 100%|██████████| 267/267 [00:13<00:00, 19.25it/s]\n",
            "accuracy: 0.3506, precision: 0.3631, recall: 0.3697, f1_measure: 0.3664, loss: 3.1811 ||: 100%|██████████| 35/35 [00:00<00:00, 91.32it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.8961, precision: 0.9215, recall: 0.9573, f1_measure: 0.9391, loss: 0.3208 ||: 100%|██████████| 267/267 [00:14<00:00, 18.83it/s]\n",
            "accuracy: 0.3597, precision: 0.3758, recall: 0.3758, f1_measure: 0.3758, loss: 3.4666 ||: 100%|██████████| 35/35 [00:00<00:00, 95.33it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.9060, precision: 0.9268, recall: 0.9534, f1_measure: 0.9399, loss: 0.2983 ||: 100%|██████████| 267/267 [00:14<00:00, 18.84it/s]\n",
            "accuracy: 0.3651, precision: 0.3907, recall: 0.3576, f1_measure: 0.3734, loss: 3.4769 ||: 100%|██████████| 35/35 [00:00<00:00, 96.34it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.9082, precision: 0.9338, recall: 0.9534, f1_measure: 0.9435, loss: 0.2908 ||: 100%|██████████| 267/267 [00:14<00:00, 20.98it/s]\n",
            "accuracy: 0.3524, precision: 0.3626, recall: 0.3758, f1_measure: 0.3690, loss: 3.5603 ||: 100%|██████████| 35/35 [00:00<00:00, 93.24it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.8986, precision: 0.9328, recall: 0.9596, f1_measure: 0.9460, loss: 0.3070 ||: 100%|██████████| 267/267 [00:14<00:00, 18.80it/s]\n",
            "accuracy: 0.3688, precision: 0.3725, recall: 0.3455, f1_measure: 0.3585, loss: 3.5392 ||: 100%|██████████| 35/35 [00:00<00:00, 96.08it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.9025, precision: 0.9310, recall: 0.9526, f1_measure: 0.9417, loss: 0.3024 ||: 100%|██████████| 267/267 [00:14<00:00, 18.89it/s]\n",
            "accuracy: 0.3660, precision: 0.3648, recall: 0.3515, f1_measure: 0.3580, loss: 3.5337 ||: 100%|██████████| 35/35 [00:00<00:00, 97.53it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.9115, precision: 0.9375, recall: 0.9557, f1_measure: 0.9466, loss: 0.2752 ||: 100%|██████████| 267/267 [00:14<00:00, 18.57it/s]\n",
            "accuracy: 0.3669, precision: 0.3584, recall: 0.3758, f1_measure: 0.3669, loss: 3.6622 ||: 100%|██████████| 35/35 [00:00<00:00, 91.74it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.9215, precision: 0.9449, recall: 0.9589, f1_measure: 0.9518, loss: 0.2544 ||: 100%|██████████| 267/267 [00:14<00:00, 18.79it/s]\n",
            "accuracy: 0.3697, precision: 0.3882, recall: 0.3576, f1_measure: 0.3722, loss: 3.7589 ||: 100%|██████████| 35/35 [00:00<00:00, 98.24it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.9218, precision: 0.9414, recall: 0.9604, f1_measure: 0.9508, loss: 0.2513 ||: 100%|██████████| 267/267 [00:14<00:00, 18.85it/s]\n",
            "accuracy: 0.3815, precision: 0.4014, recall: 0.3455, f1_measure: 0.3713, loss: 3.8189 ||: 100%|██████████| 35/35 [00:00<00:00, 94.74it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.9261, precision: 0.9456, recall: 0.9589, f1_measure: 0.9522, loss: 0.2365 ||: 100%|██████████| 267/267 [00:14<00:00, 18.78it/s]\n",
            "accuracy: 0.3542, precision: 0.3675, recall: 0.3697, f1_measure: 0.3686, loss: 3.8991 ||: 100%|██████████| 35/35 [00:00<00:00, 94.60it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.9244, precision: 0.9531, recall: 0.9620, f1_measure: 0.9575, loss: 0.2428 ||: 100%|██████████| 267/267 [00:14<00:00, 16.82it/s]\n",
            "accuracy: 0.3533, precision: 0.3722, recall: 0.4061, f1_measure: 0.3884, loss: 4.0002 ||: 100%|██████████| 35/35 [00:00<00:00, 90.74it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.9276, precision: 0.9436, recall: 0.9620, f1_measure: 0.9527, loss: 0.2313 ||: 100%|██████████| 267/267 [00:14<00:00, 19.02it/s]\n",
            "accuracy: 0.3678, precision: 0.3931, recall: 0.3455, f1_measure: 0.3677, loss: 3.9712 ||: 100%|██████████| 35/35 [00:00<00:00, 96.23it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.9259, precision: 0.9373, recall: 0.9511, f1_measure: 0.9441, loss: 0.2385 ||: 100%|██████████| 267/267 [00:14<00:00, 15.66it/s]\n",
            "accuracy: 0.3588, precision: 0.3636, recall: 0.3636, f1_measure: 0.3636, loss: 4.0249 ||: 100%|██████████| 35/35 [00:00<00:00, 97.34it/s] \n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.9319, precision: 0.9483, recall: 0.9689, f1_measure: 0.9585, loss: 0.2174 ||: 100%|██████████| 267/267 [00:14<00:00, 18.98it/s]\n",
            "accuracy: 0.3678, precision: 0.3782, recall: 0.3576, f1_measure: 0.3676, loss: 4.1777 ||: 100%|██████████| 35/35 [00:00<00:00, 97.61it/s] \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_epoch': 6,\n",
              " 'best_validation_accuracy': 0.371480472297911,\n",
              " 'best_validation_f1_measure': 0.4157894551753998,\n",
              " 'best_validation_loss': 1.4733514547348023,\n",
              " 'best_validation_precision': 0.367441862821579,\n",
              " 'best_validation_recall': 0.4787878692150116,\n",
              " 'epoch': 39,\n",
              " 'peak_cpu_memory_MB': 526.748,\n",
              " 'training_accuracy': 0.9318820224719101,\n",
              " 'training_cpu_memory_MB': 526.748,\n",
              " 'training_duration': '0:09:40.323243',\n",
              " 'training_epochs': 39,\n",
              " 'training_f1_measure': 0.9585253596305847,\n",
              " 'training_loss': 0.21736958931978054,\n",
              " 'training_precision': 0.9483282566070557,\n",
              " 'training_recall': 0.9689440727233887,\n",
              " 'training_start_epoch': 0,\n",
              " 'validation_accuracy': 0.3678474114441417,\n",
              " 'validation_f1_measure': 0.3676012456417084,\n",
              " 'validation_loss': 4.177747978482928,\n",
              " 'validation_precision': 0.3782051205635071,\n",
              " 'validation_recall': 0.35757574439048767}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFacAYSlPpHw",
        "colab_type": "text"
      },
      "source": [
        "## Sanity Check\n",
        "\n",
        "The cell below will allow you to enter sample sentences and test the predictions of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oxRELM_-cako",
        "outputId": "ec45c41c-21b1-464a-9d5d-e1ac02d013f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
        "logits = predictor.predict(\"Don't waste your money\")['logits']\n",
        "label_id = np.argmax(logits)\n",
        "\n",
        "print(model.vocab.get_token_from_index(label_id, 'labels'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wiZLRRYPpIJ",
        "colab_type": "text"
      },
      "source": [
        "## More Substantive Checks\n",
        "\n",
        "In order to do some more in depth checks how well the model does, and how well it might generalize we can utilize a set of Amazon reviews. \n",
        "\n",
        "http://jmcauley.ucsd.edu/data/amazon/\n",
        "\n",
        "The site above holds a very large of Amazon reviews that can be used for scientific purposes. \n",
        "\n",
        "### Task 1: Choose and Download a Subcategory\n",
        "\n",
        "From the table below, choose a category that you will use for testing. \n",
        "Download the 5 core links that hold the full text, title and rating of a review. \n",
        "\n",
        "\n",
        "<html>\n",
        "\n",
        "<table>\n",
        "<tbody><tr>\n",
        "  <td>Books</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_10.json.gz\">10-core</a> (4,701,968 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz\">5-core</a> (8,898,041 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Books.csv\">ratings only</a> (22,507,155 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Electronics</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_10.json.gz\">10-core</a> (347,393 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\">5-core</a> (1,689,188 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Electronics.csv\">ratings only</a> (7,824,482 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Movies and TV</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Movies_and_TV_10.json.gz\">10-core</a> (958,986 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Movies_and_TV_5.json.gz\">5-core</a> (1,697,533 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Movies_and_TV.csv\">ratings only</a> (4,607,047 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>CDs and Vinyl</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_CDs_and_Vinyl_10.json.gz\">10-core</a> (445,412 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_CDs_and_Vinyl_5.json.gz\">5-core</a> (1,097,592 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_CDs_and_Vinyl.csv\">ratings only</a> (3,749,004 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Clothing, Shoes and Jewelry</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Clothing_Shoes_and_Jewelry_5.json.gz\">5-core</a> (278,677 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Clothing_Shoes_and_Jewelry.csv\">ratings only</a> (5,748,920 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Home and Kitchen</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Home_and_Kitchen_10.json.gz\">10-core</a> (25,445 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Home_and_Kitchen_5.json.gz\">5-core</a> (551,682 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Home_and_Kitchen.csv\">ratings only</a> (4,253,926 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Kindle Store</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Kindle_Store_10.json.gz\">10-core</a> (367,478 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Kindle_Store_5.json.gz\">5-core</a> (982,619 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Kindle_Store.csv\">ratings only</a> (3,205,467 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Sports and Outdoors</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Sports_and_Outdoors_5.json.gz\">5-core</a> (296,337 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Sports_and_Outdoors.csv\">ratings only</a> (3,268,695 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Cell Phones and Accessories</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_10.json.gz\">10-core</a> (1,854 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz\">5-core</a> (194,439 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Cell_Phones_and_Accessories.csv\">ratings only</a> (3,447,249 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Health and Personal Care</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Health_and_Personal_Care_10.json.gz\">10-core</a> (55,076 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Health_and_Personal_Care_5.json.gz\">5-core</a> (346,355 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Health_and_Personal_Care.csv\">ratings only</a> (2,982,326 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Toys and Games</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Toys_and_Games_10.json.gz\">10-core</a> (18,637 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Toys_and_Games_5.json.gz\">5-core</a> (167,597 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Toys_and_Games.csv\">ratings only</a> (2,252,771 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Video Games</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Video_Games_10.json.gz\">10-core</a> (52,158 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Video_Games_5.json.gz\">5-core</a> (231,780 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Video_Games.csv\">ratings only</a> (1,324,753 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Tools and Home Improvement</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Tools_and_Home_Improvement_5.json.gz\">5-core</a> (134,476 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Tools_and_Home_Improvement.csv\">ratings only</a> (1,926,047 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Beauty</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Beauty_10.json.gz\">10-core</a> (28,798 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Beauty_5.json.gz\">5-core</a> (198,502 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Beauty.csv\">ratings only</a> (2,023,070 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Apps for Android</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Apps_for_Android_10.json.gz\">10-core</a> (264,050 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Apps_for_Android_5.json.gz\">5-core</a> (752,937 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Apps_for_Android.csv\">ratings only</a> (2,638,172 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Office Products</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Office_Products_10.json.gz\">10-core</a> (25,374 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Office_Products_5.json.gz\">5-core</a> (53,258 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Office_Products.csv\">ratings only</a> (1,243,186 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Pet Supplies</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Pet_Supplies_10.json.gz\">10-core</a> (3,152 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Pet_Supplies_5.json.gz\">5-core</a> (157,836 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Pet_Supplies.csv\">ratings only</a> (1,235,316 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Automotive</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Automotive_5.json.gz\">5-core</a> (20,473 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Automotive.csv\">ratings only</a> (1,373,768 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Grocery and Gourmet Food</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Grocery_and_Gourmet_Food_10.json.gz\">10-core</a> (37,348 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Grocery_and_Gourmet_Food_5.json.gz\">5-core</a> (151,254 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Grocery_and_Gourmet_Food.csv\">ratings only</a> (1,297,156 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Patio, Lawn and Garden</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Patio_Lawn_and_Garden_5.json.gz\">5-core</a> (13,272 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Patio_Lawn_and_Garden.csv\">ratings only</a> (993,490 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Baby</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Baby_5.json.gz\">5-core</a> (160,792 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Baby.csv\">ratings only</a> (915,446 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Digital Music</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Digital_Music_10.json.gz\">10-core</a> (22,772 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Digital_Music_5.json.gz\">5-core</a> (64,706 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Digital_Music.csv\">ratings only</a> (836,006 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Musical Instruments</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz\">5-core</a> (10,261 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Musical_Instruments.csv\">ratings only</a> (500,176 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Amazon Instant Video</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Amazon_Instant_Video_5.json.gz\">5-core</a> (37,126 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Amazon_Instant_Video.csv\">ratings only</a> (583,933 ratings)</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "    </table>\n",
        "</html>\n",
        "\n",
        "### Task 2\n",
        "\n",
        "Sanity checks with the reviews.\n",
        "\n",
        "The format of the files is as follows:\n",
        "\n",
        "`{\n",
        "\t\"reviewerID\": \"A2ICI6VUC0U5K6\",\n",
        "\t\"asin\": \"B0014JKKGK\",\n",
        "\t\"reviewerName\": \"Jermin Botrous \\\"gigigigi\\\"\",\n",
        "\t\"helpful\": [0, 0],\n",
        "\t\"reviewText\": \"Don't waste your money because elastic goes bad after 2 washes\",\n",
        "\t\"overall\": 1.0,\n",
        "\t\"summary\": \"One Star\",\n",
        "\t\"unixReviewTime\": 1404432000,\n",
        "\t\"reviewTime\": \"07 4, 2014\"\n",
        "}`\n",
        "\n",
        "Use the following code snippets to load individal review texts.\n",
        "\n",
        "Opening a file in python:\n",
        "\n",
        "``\n",
        "test_file = open('file_name.json', 'r')\n",
        "first_line = test_file.readline()\n",
        "``\n",
        "\n",
        "Transform the line into a json object to access the individual fiels (such as reviewText).\n",
        "\n",
        "\n",
        "``\n",
        "import json\n",
        "j_obj = json.loads(first_line)\n",
        "print('reviewText:' + j_obj['reviewText'])\n",
        "``\n",
        "\n",
        "finally use the code from above to test the predictions\n",
        "\n",
        "``\n",
        "logits = predictor.predict(\"Don't waste your money\")['logits']\n",
        "label_id = np.argmax(logits)\n",
        "prediction = model.vocab.get_token_from_index(label_id, 'labels')\n",
        "print(prediction)\n",
        "``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qCs1IUoPpIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "83f1f268-c90f-4797-f4e7-8bd3943a838a"
      },
      "source": [
        "test_file = open('../reviews_Digital_Music_5.json', 'r')\n",
        "first_line = test_file.readline()\n",
        "print(first_line)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"reviewerID\": \"A3EBHHCZO6V2A4\", \"asin\": \"5555991584\", \"reviewerName\": \"Amaranth \\\"music fan\\\"\", \"helpful\": [3, 3], \"reviewText\": \"It's hard to believe \\\"Memory of Trees\\\" came out 11 years ago;it has held up well over the passage of time.It's Enya's last great album before the New Age/pop of \\\"Amarantine\\\" and \\\"Day without rain.\\\" Back in 1995,Enya still had her creative spark,her own voice.I agree with the reviewer who said that this is her saddest album;it is melancholy,bittersweet,from the opening title song.\\\"Memory of Trees\\\" is elegaic&majestic.;\\\"Pax Deorum\\\" sounds like it is from a Requiem Mass,it is a dark threnody.Unlike the reviewer who said that this has a \\\"disconcerting\\\" blend of spirituality&sensuality;,I don't find it disconcerting at all.\\\"Anywhere is\\\" is a hopeful song,looking to possibilities.\\\"Hope has a place\\\" is about love,but it is up to the listener to decide if it is romantic,platonic,etc.I've always had a soft spot for this song.\\\"On my way home\\\" is a triumphant ending about return.This is truly a masterpiece of New Age music,a must for any Enya fan!\", \"overall\": 5.0, \"summary\": \"Enya's last great album\", \"unixReviewTime\": 1158019200, \"reviewTime\": \"09 12, 2006\"}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdzxAtyzb9w0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d785f0bf-632b-468e-8620-4af7630df7a1"
      },
      "source": [
        "import json \n",
        "j_obj = json.loads(first_line) \n",
        "print('reviewText:' + j_obj['reviewText'])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reviewText:It's hard to believe \"Memory of Trees\" came out 11 years ago;it has held up well over the passage of time.It's Enya's last great album before the New Age/pop of \"Amarantine\" and \"Day without rain.\" Back in 1995,Enya still had her creative spark,her own voice.I agree with the reviewer who said that this is her saddest album;it is melancholy,bittersweet,from the opening title song.\"Memory of Trees\" is elegaic&majestic.;\"Pax Deorum\" sounds like it is from a Requiem Mass,it is a dark threnody.Unlike the reviewer who said that this has a \"disconcerting\" blend of spirituality&sensuality;,I don't find it disconcerting at all.\"Anywhere is\" is a hopeful song,looking to possibilities.\"Hope has a place\" is about love,but it is up to the listener to decide if it is romantic,platonic,etc.I've always had a soft spot for this song.\"On my way home\" is a triumphant ending about return.This is truly a masterpiece of New Age music,a must for any Enya fan!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4ohLt95caa4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16fbbc11-3d3d-4b40-daff-9bde4e7c06c9"
      },
      "source": [
        "logits = predictor.predict(\"Don't waste your money\")['logits']\n",
        "label_id = np.argmax(logits)\n",
        "prediction = model.vocab.get_token_from_index(label_id, 'labels')\n",
        "print(prediction)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjtOtcz0PpIQ",
        "colab_type": "text"
      },
      "source": [
        "## Task 3\n",
        "\n",
        "Iterate over the reviews and extract:\n",
        "\n",
        "* 100 positive predictions (i.e. 4)\n",
        "* 100 negative predictions (i.e. 0)\n",
        "\n",
        "Save the sets of positive and negative predictions as plain text files:\n",
        "\n",
        "* categoryName_100_pos.txt\n",
        "* categoryName_100_neg.txt\n",
        "\n",
        "Manually inspect the predictions to identify potential false positives in boths sets.\n",
        "Store a couple of those false positives in the files:\n",
        "\n",
        "* categoryName_100_pos_fp.txt\n",
        "* categoryName_100_neg_fp.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9tJNPo7PpIR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "203a0beb-3617-42c4-e905-0c913b9bd2ad"
      },
      "source": [
        "test_file = open('../reviews_Digital_Music_5.json', 'r')\n",
        "lines = test_file.readlines()\n",
        "positives = []\n",
        "negatives = []\n",
        "for line in lines:\n",
        "  j_obj = json.loads(line) \n",
        "  logits = predictor.predict(j_obj['reviewText'])['logits']\n",
        "  label_id = np.argmax(logits)\n",
        "  prediction = model.vocab.get_token_from_index(label_id, 'labels')\n",
        "  print(prediction)\n",
        "  if prediction == '4' and len(positives) < 100:\n",
        "    positives.append(line)\n",
        "    print(\"Pos len : \"+ str(len(positives)))\n",
        "  elif prediction == '0' and len(negatives) < 100:\n",
        "    negatives.append(line)\n",
        "    print(\"Neg len : \"+ str(len(negatives)))\n",
        "  if len(positives) >= 100 and len(negatives) >= 100:\n",
        "    break\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "Pos len : 1\n",
            "4\n",
            "Pos len : 2\n",
            "1\n",
            "3\n",
            "3\n",
            "4\n",
            "Pos len : 3\n",
            "4\n",
            "Pos len : 4\n",
            "0\n",
            "Neg len : 1\n",
            "4\n",
            "Pos len : 5\n",
            "2\n",
            "3\n",
            "2\n",
            "1\n",
            "2\n",
            "4\n",
            "Pos len : 6\n",
            "2\n",
            "0\n",
            "Neg len : 2\n",
            "4\n",
            "Pos len : 7\n",
            "3\n",
            "0\n",
            "Neg len : 3\n",
            "1\n",
            "4\n",
            "Pos len : 8\n",
            "4\n",
            "Pos len : 9\n",
            "4\n",
            "Pos len : 10\n",
            "3\n",
            "0\n",
            "Neg len : 4\n",
            "2\n",
            "0\n",
            "Neg len : 5\n",
            "2\n",
            "1\n",
            "0\n",
            "Neg len : 6\n",
            "2\n",
            "1\n",
            "1\n",
            "4\n",
            "Pos len : 11\n",
            "0\n",
            "Neg len : 7\n",
            "3\n",
            "4\n",
            "Pos len : 12\n",
            "3\n",
            "3\n",
            "3\n",
            "4\n",
            "Pos len : 13\n",
            "4\n",
            "Pos len : 14\n",
            "0\n",
            "Neg len : 8\n",
            "2\n",
            "3\n",
            "0\n",
            "Neg len : 9\n",
            "2\n",
            "4\n",
            "Pos len : 15\n",
            "3\n",
            "0\n",
            "Neg len : 10\n",
            "3\n",
            "4\n",
            "Pos len : 16\n",
            "2\n",
            "0\n",
            "Neg len : 11\n",
            "3\n",
            "3\n",
            "4\n",
            "Pos len : 17\n",
            "1\n",
            "1\n",
            "0\n",
            "Neg len : 12\n",
            "3\n",
            "1\n",
            "3\n",
            "0\n",
            "Neg len : 13\n",
            "3\n",
            "0\n",
            "Neg len : 14\n",
            "4\n",
            "Pos len : 18\n",
            "0\n",
            "Neg len : 15\n",
            "3\n",
            "1\n",
            "1\n",
            "2\n",
            "0\n",
            "Neg len : 16\n",
            "1\n",
            "3\n",
            "1\n",
            "1\n",
            "1\n",
            "3\n",
            "4\n",
            "Pos len : 19\n",
            "3\n",
            "2\n",
            "4\n",
            "Pos len : 20\n",
            "2\n",
            "3\n",
            "2\n",
            "4\n",
            "Pos len : 21\n",
            "0\n",
            "Neg len : 17\n",
            "1\n",
            "0\n",
            "Neg len : 18\n",
            "1\n",
            "3\n",
            "3\n",
            "0\n",
            "Neg len : 19\n",
            "0\n",
            "Neg len : 20\n",
            "0\n",
            "Neg len : 21\n",
            "1\n",
            "3\n",
            "1\n",
            "1\n",
            "1\n",
            "3\n",
            "4\n",
            "Pos len : 22\n",
            "1\n",
            "4\n",
            "Pos len : 23\n",
            "1\n",
            "0\n",
            "Neg len : 22\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "0\n",
            "Neg len : 23\n",
            "3\n",
            "1\n",
            "2\n",
            "4\n",
            "Pos len : 24\n",
            "1\n",
            "2\n",
            "0\n",
            "Neg len : 24\n",
            "1\n",
            "1\n",
            "0\n",
            "Neg len : 25\n",
            "3\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "3\n",
            "4\n",
            "Pos len : 25\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "4\n",
            "Pos len : 26\n",
            "0\n",
            "Neg len : 26\n",
            "2\n",
            "4\n",
            "Pos len : 27\n",
            "0\n",
            "Neg len : 27\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "4\n",
            "Pos len : 28\n",
            "3\n",
            "1\n",
            "3\n",
            "1\n",
            "0\n",
            "Neg len : 28\n",
            "1\n",
            "4\n",
            "Pos len : 29\n",
            "2\n",
            "3\n",
            "4\n",
            "Pos len : 30\n",
            "4\n",
            "Pos len : 31\n",
            "2\n",
            "0\n",
            "Neg len : 29\n",
            "3\n",
            "4\n",
            "Pos len : 32\n",
            "4\n",
            "Pos len : 33\n",
            "4\n",
            "Pos len : 34\n",
            "3\n",
            "3\n",
            "4\n",
            "Pos len : 35\n",
            "3\n",
            "4\n",
            "Pos len : 36\n",
            "0\n",
            "Neg len : 30\n",
            "4\n",
            "Pos len : 37\n",
            "0\n",
            "Neg len : 31\n",
            "1\n",
            "4\n",
            "Pos len : 38\n",
            "2\n",
            "4\n",
            "Pos len : 39\n",
            "0\n",
            "Neg len : 32\n",
            "4\n",
            "Pos len : 40\n",
            "4\n",
            "Pos len : 41\n",
            "1\n",
            "0\n",
            "Neg len : 33\n",
            "3\n",
            "0\n",
            "Neg len : 34\n",
            "2\n",
            "3\n",
            "4\n",
            "Pos len : 42\n",
            "4\n",
            "Pos len : 43\n",
            "1\n",
            "3\n",
            "1\n",
            "3\n",
            "3\n",
            "3\n",
            "1\n",
            "2\n",
            "4\n",
            "Pos len : 44\n",
            "0\n",
            "Neg len : 35\n",
            "4\n",
            "Pos len : 45\n",
            "4\n",
            "Pos len : 46\n",
            "3\n",
            "1\n",
            "2\n",
            "1\n",
            "0\n",
            "Neg len : 36\n",
            "1\n",
            "3\n",
            "2\n",
            "4\n",
            "Pos len : 47\n",
            "0\n",
            "Neg len : 37\n",
            "3\n",
            "4\n",
            "Pos len : 48\n",
            "4\n",
            "Pos len : 49\n",
            "4\n",
            "Pos len : 50\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "3\n",
            "3\n",
            "3\n",
            "1\n",
            "4\n",
            "Pos len : 51\n",
            "2\n",
            "3\n",
            "1\n",
            "1\n",
            "4\n",
            "Pos len : 52\n",
            "0\n",
            "Neg len : 38\n",
            "1\n",
            "0\n",
            "Neg len : 39\n",
            "3\n",
            "0\n",
            "Neg len : 40\n",
            "4\n",
            "Pos len : 53\n",
            "4\n",
            "Pos len : 54\n",
            "2\n",
            "3\n",
            "4\n",
            "Pos len : 55\n",
            "1\n",
            "4\n",
            "Pos len : 56\n",
            "3\n",
            "0\n",
            "Neg len : 41\n",
            "4\n",
            "Pos len : 57\n",
            "2\n",
            "4\n",
            "Pos len : 58\n",
            "3\n",
            "4\n",
            "Pos len : 59\n",
            "2\n",
            "4\n",
            "Pos len : 60\n",
            "4\n",
            "Pos len : 61\n",
            "2\n",
            "2\n",
            "4\n",
            "Pos len : 62\n",
            "2\n",
            "4\n",
            "Pos len : 63\n",
            "4\n",
            "Pos len : 64\n",
            "3\n",
            "1\n",
            "3\n",
            "3\n",
            "4\n",
            "Pos len : 65\n",
            "3\n",
            "3\n",
            "4\n",
            "Pos len : 66\n",
            "0\n",
            "Neg len : 42\n",
            "3\n",
            "4\n",
            "Pos len : 67\n",
            "3\n",
            "3\n",
            "3\n",
            "0\n",
            "Neg len : 43\n",
            "4\n",
            "Pos len : 68\n",
            "0\n",
            "Neg len : 44\n",
            "3\n",
            "3\n",
            "3\n",
            "4\n",
            "Pos len : 69\n",
            "3\n",
            "3\n",
            "2\n",
            "2\n",
            "4\n",
            "Pos len : 70\n",
            "3\n",
            "0\n",
            "Neg len : 45\n",
            "4\n",
            "Pos len : 71\n",
            "0\n",
            "Neg len : 46\n",
            "4\n",
            "Pos len : 72\n",
            "3\n",
            "3\n",
            "4\n",
            "Pos len : 73\n",
            "3\n",
            "1\n",
            "3\n",
            "3\n",
            "4\n",
            "Pos len : 74\n",
            "3\n",
            "4\n",
            "Pos len : 75\n",
            "0\n",
            "Neg len : 47\n",
            "4\n",
            "Pos len : 76\n",
            "4\n",
            "Pos len : 77\n",
            "1\n",
            "4\n",
            "Pos len : 78\n",
            "0\n",
            "Neg len : 48\n",
            "0\n",
            "Neg len : 49\n",
            "2\n",
            "0\n",
            "Neg len : 50\n",
            "4\n",
            "Pos len : 79\n",
            "1\n",
            "4\n",
            "Pos len : 80\n",
            "0\n",
            "Neg len : 51\n",
            "0\n",
            "Neg len : 52\n",
            "3\n",
            "3\n",
            "1\n",
            "2\n",
            "3\n",
            "1\n",
            "1\n",
            "4\n",
            "Pos len : 81\n",
            "2\n",
            "1\n",
            "2\n",
            "4\n",
            "Pos len : 82\n",
            "0\n",
            "Neg len : 53\n",
            "2\n",
            "0\n",
            "Neg len : 54\n",
            "0\n",
            "Neg len : 55\n",
            "4\n",
            "Pos len : 83\n",
            "1\n",
            "0\n",
            "Neg len : 56\n",
            "0\n",
            "Neg len : 57\n",
            "4\n",
            "Pos len : 84\n",
            "0\n",
            "Neg len : 58\n",
            "2\n",
            "2\n",
            "3\n",
            "1\n",
            "1\n",
            "4\n",
            "Pos len : 85\n",
            "4\n",
            "Pos len : 86\n",
            "1\n",
            "3\n",
            "0\n",
            "Neg len : 59\n",
            "2\n",
            "0\n",
            "Neg len : 60\n",
            "1\n",
            "3\n",
            "1\n",
            "4\n",
            "Pos len : 87\n",
            "2\n",
            "3\n",
            "0\n",
            "Neg len : 61\n",
            "1\n",
            "3\n",
            "2\n",
            "0\n",
            "Neg len : 62\n",
            "0\n",
            "Neg len : 63\n",
            "0\n",
            "Neg len : 64\n",
            "0\n",
            "Neg len : 65\n",
            "1\n",
            "3\n",
            "3\n",
            "1\n",
            "3\n",
            "0\n",
            "Neg len : 66\n",
            "0\n",
            "Neg len : 67\n",
            "0\n",
            "Neg len : 68\n",
            "4\n",
            "Pos len : 88\n",
            "0\n",
            "Neg len : 69\n",
            "1\n",
            "4\n",
            "Pos len : 89\n",
            "1\n",
            "0\n",
            "Neg len : 70\n",
            "2\n",
            "0\n",
            "Neg len : 71\n",
            "2\n",
            "1\n",
            "1\n",
            "0\n",
            "Neg len : 72\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "4\n",
            "Pos len : 90\n",
            "1\n",
            "3\n",
            "0\n",
            "Neg len : 73\n",
            "0\n",
            "Neg len : 74\n",
            "3\n",
            "2\n",
            "4\n",
            "Pos len : 91\n",
            "0\n",
            "Neg len : 75\n",
            "2\n",
            "1\n",
            "0\n",
            "Neg len : 76\n",
            "0\n",
            "Neg len : 77\n",
            "2\n",
            "0\n",
            "Neg len : 78\n",
            "1\n",
            "2\n",
            "0\n",
            "Neg len : 79\n",
            "3\n",
            "0\n",
            "Neg len : 80\n",
            "0\n",
            "Neg len : 81\n",
            "3\n",
            "4\n",
            "Pos len : 92\n",
            "3\n",
            "4\n",
            "Pos len : 93\n",
            "4\n",
            "Pos len : 94\n",
            "4\n",
            "Pos len : 95\n",
            "1\n",
            "0\n",
            "Neg len : 82\n",
            "0\n",
            "Neg len : 83\n",
            "0\n",
            "Neg len : 84\n",
            "1\n",
            "2\n",
            "1\n",
            "3\n",
            "2\n",
            "4\n",
            "Pos len : 96\n",
            "0\n",
            "Neg len : 85\n",
            "0\n",
            "Neg len : 86\n",
            "2\n",
            "1\n",
            "0\n",
            "Neg len : 87\n",
            "0\n",
            "Neg len : 88\n",
            "0\n",
            "Neg len : 89\n",
            "1\n",
            "1\n",
            "3\n",
            "1\n",
            "3\n",
            "3\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "4\n",
            "Pos len : 97\n",
            "4\n",
            "Pos len : 98\n",
            "3\n",
            "0\n",
            "Neg len : 90\n",
            "0\n",
            "Neg len : 91\n",
            "2\n",
            "3\n",
            "1\n",
            "3\n",
            "3\n",
            "1\n",
            "2\n",
            "4\n",
            "Pos len : 99\n",
            "1\n",
            "0\n",
            "Neg len : 92\n",
            "3\n",
            "4\n",
            "Pos len : 100\n",
            "1\n",
            "1\n",
            "0\n",
            "Neg len : 93\n",
            "1\n",
            "2\n",
            "3\n",
            "1\n",
            "3\n",
            "2\n",
            "2\n",
            "1\n",
            "0\n",
            "Neg len : 94\n",
            "0\n",
            "Neg len : 95\n",
            "0\n",
            "Neg len : 96\n",
            "2\n",
            "0\n",
            "Neg len : 97\n",
            "4\n",
            "3\n",
            "4\n",
            "3\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "Neg len : 98\n",
            "1\n",
            "2\n",
            "0\n",
            "Neg len : 99\n",
            "2\n",
            "0\n",
            "Neg len : 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J6mrvZFhtkM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "60fba012-adc4-4ea6-ea97-52332bb0beec"
      },
      "source": [
        "print(positives)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['{\"reviewerID\": \"A3O90G1D7I5EGG\", \"asin\": \"5555991584\", \"reviewerName\": \"dev1\", \"helpful\": [1, 5], \"reviewText\": \"Enya is one of a few artists whom I consider successful at creating this type of electronic atmospheric and dreamlike music. For certain, The Memory Of Trees is melodic, romantic and sensuous. Roma Ryan\\'s lyrics, sung in  Gaelic by Enya (Athair Ar Neamh), float gently like pillow-soft white  clouds. The compositions are relaxing and harmonious (Tea House Moon),  dreamlike and filled with mystery (Once You Had Gold), and include subtle  touches of Celtic tradition (Enya\\'s phrasing on `Hope Has A Place\\'). The CD  is a quiet and lush alternative to ninety-percent of the popular music on  the market today, and also a welcome substitute for the often hectic pace  of daily life. Enya\\'s arrangement of the compositions on Memory is well  thought - the comparatively dramatic and uplifting `Anywhere Is\\' and `On My  Way Home\\' are placed so that the listener doesn\\'t fall asleep.However,  I\\'m very leery when I read a dozen reviews of a particular musical release,  and all the writers rate the artist\\'s work with five-stars. My hesitation  is not a matter of taste (listeners are free to condemn or rave about  whatever they please), but one of perspective. Is The Memory Of Trees a  musical ground breaking effort? Does it introduce a unique technique or  significantly alter a current genre of popular music? On close listening,  the electronic pseudo-symphonic music of Memory does not approach the  beauty or dynamics of an orchestra with real people playing real  instruments. Nor does Enya\\'s multi-layering of her own voice have the same  rich sonic texture as a chorus comprised of real voices. Then again,  perhaps The Memory Of Trees (and all of Enya\\'s work) is so provocatively  ethereal because it is surreal.\", \"overall\": 3.0, \"summary\": \"Have You Hugged A Tree Today?\", \"unixReviewTime\": 959385600, \"reviewTime\": \"05 27, 2000\"}\\n', '{\"reviewerID\": \"A3QMJMTLJC34QC\", \"asin\": \"B0000002O5\", \"reviewerName\": \"Vincent E. Martin \\\\\"Darkbard\\\\\"\", \"helpful\": [3, 3], \"reviewText\": \"Bottom-Line: All and all \\\\\"So Long, So Wrong\\\\\" is a 3.5 star effort for AKUS because of the aforementioned vocal track issue.Despite the fact that Alison Krauss hasn\\'t the vocal range of most in Country & Western/Blue Grass, she has managed to become the voice of the latter.  Her voice is lithe, but with a charm and genuine cadence that is hard to resist; hence her success, mostly with her band Union Station.In 1995 the band Alison Krauss & Union Station (AKUS) released their seventh album \\\\\"So Long, So Wrong,\\\\\" which went on to garner critical acclaim, but not very much commercial success.  As is the band\\'s style they stayed away from the lavishly produced, pseudo-C&W; Pop that so marks other groups their adopted genre and instead remain fairly close to their Bluegrass roots.  But perhaps the CD was under-produced and engineered if there is such a malady.Most of the album\\'s vocal tracks were too quiet; i.e. it was hard to understand the lyrics throughout most of the CD especially when Ms. Krauss was taking lead.  The especially true on the CD\\'s title track, So Long, So Wrong (track No. 1), wherein Ms. Kruass\\'s already lithe voice is at points inaudible.  The same holds true for track No. 2 No Place To Hide, track No. 3 Deeper Than Crying, but to a lesser extent, track No. 4 I Can Let Go Now, track No. 5 The Road is a Lover, track No. 7 It Doesn\\'t Matter, well you get the picture.  The musicianship was flawless as usual, but the quiet vocal tracks detracted from my overall enjoyment of the CD.That is not to say the entire album is a throwaway because of the vocal shortcomings, far from it.  \\\\\"So Long, So Wrong\\\\\" maintains the bands tradition of fine musicianship and crisp clear harmonizing vocals (when they can be heard that is) and is overall a worthy AKUS release; just not my favorite.When Krauss isn\\'t spotlighted on center stage, her presence is still apparent in the guise of her high (soprano) harmonies and energetic fiddle.  Her ability to blend into the group, fade into the body as it were, only buttresses the sense that AKUS are indeed a band rather than merely a supporting backdrop for Krauss\\'s solo efforts.  An equaling male vocal cord is struck by guitarist and long time band member Dan Tyminski, who brings more traditional bluegrass sound to the group.  And Ron Block, who is not the strongest (solo) vocalists is an excellent song writer, and contributed two track to the album, track No. 11 Pain of a Troubled Mind (lead vocals) and track No. 14, the hauntingly beautiful There is a Reason.\\\\\"So Long, So Wrong\\\\\" is an ambitious CD; there are fourteen vocal tracks and one instrumental, track No. 6 Little Liza Jane.  When released the song were included on a two-album set, but only one CD.  All of the members of AKUS play instruments, indeed they are all considered virtuosos.  The circa 1997 group included Alison Krauss (vocals, fiddle, violin), Barry Bales (acoustic bass), Ron Block (banjo, guitar, vocals), Adam Steffey (mandolin), and Dan Tyminski (guitar, mandolin, vocals).  Adam Steffey has since left the group to be replaced by Jerry Douglas on the dobro.All and all \\\\\"So Long, So Wrong\\\\\" is a 3.5 star effort for AKUS because of the aforementioned vocal track issue.  The instrumentation on the CD is top notch and more than part of the reason I love this group so much; they have such a passion for Bluegrass and make this quintessential form of American music swell the soul.\", \"overall\": 3.0, \"summary\": \"Not One of My Favorite AKUS Albums, but Still Laudable\", \"unixReviewTime\": 1294531200, \"reviewTime\": \"01 9, 2011\"}\\n', '{\"reviewerID\": \"A2DVFHG099GUGE\", \"asin\": \"B0000004TW\", \"reviewerName\": \"sauerkraut\", \"helpful\": [10, 13], \"reviewText\": \"I\\'ve always had a lot of respect for A Flock of Seagulls.  These guys have written interesting and catchy music with imagination.  I like the way they combine the keyboard and guitar playing.  Each instrument complements the  other quite nicely.  This debut album from A Flock of Seagulls is very  solid and energetic.  It\\'s in a new wave synth pop direction with an equal  dose of guitar playing.  All ten of the tracks are satisfying.  The album\\'s  musicianship, songwriting, and production are good.  My favorite tunes are  &quot;I Ran,&quot; &quot;Space Age Love Song,&quot; and &quot;D.N.A.&quot;  &quot;Space Age Lovesong&quot; is my favorite song from A Flock of  Seagulls.  That tune has imagination and catchiness all rolled into one.  Mike Score\\'s vocals really soar on it.  It\\'s truly a well-written song.  Mike Score does a nice job with the vocals throughout the album.  His voice  is effective and unique.  &quot;D.N.A.&quot; happens to be a very  impressive instrumental.  Paul Reynolds\\' guitar work on it is really good.  &quot;I Ran&quot; is also a cool and memorable tune.  &quot;Modern Love is  Automatic&quot; has an interesting intro.  This album has both futuristic  and imaginative qualities.  It\\'s worth a listen.\", \"overall\": 3.0, \"summary\": \"Enjoyable from beginning to end.\", \"unixReviewTime\": 964396800, \"reviewTime\": \"07 24, 2000\"}\\n', '{\"reviewerID\": \"AY3QGA5EZWU44\", \"asin\": \"B00000053X\", \"reviewerName\": \"Jennifer Cruz\", \"helpful\": [2, 4], \"reviewText\": \"The Only Reason I gave this C.D a chance was because my Girlfriend Happens to love these Moronic disgraces of Music. 4 songs on this 12 song album were Decent! the rest Were shoddy Production and too Poppy. &quot;quit Playing Games with my heart&quot; a great song has an rnB feel to it. &quot;As long as you love me&quot; Great Lyrics and the groove is catchy!. &quot;all i have to give&quot; fairly decent i have to be in the mood to listen to it. &quot;everybody&quot; the only reason i like this damn song is because i saw the video so many times i have the lyrics imbedded in my brain. The rest suck. Im sorry But after Artist like &quot;Prince&quot;, &quot;Stevie Wonder&quot; , &quot;Michael Jackson&quot; Paved the way for these young swill Merchants. You think Anyone Would make good music.  Well These Puppets for the record companies Suck no Punn intended. if you like this kind of music go listen to Savage Garden at least they have Talent. these morons don\\'t and Im out!\", \"overall\": 2.0, \"summary\": \"Thank god they are no More\", \"unixReviewTime\": 1065139200, \"reviewTime\": \"10 3, 2003\"}\\n', '{\"reviewerID\": \"A373KGHJIJWJCP\", \"asin\": \"B00000054H\", \"reviewerName\": \"Sean Currie (hypestyle@yahoo.com)\", \"helpful\": [0, 2], \"reviewText\": \"But did you expect any less?Most blazing cuts- &quot;The MC&quot;, produced by newcomer Domingo; &quot;A Friend&quot;, knocked out by Showbiz; &quot;HIPHOP&quot;, featuring Thor-El.  Rare promotional copies may include  &quot;Stop Scheming&quot; featuring R&amp;B crooner, Joe.&quot;Step Into  A World&quot; is featured twice- first in the Jesse West version, and then  the ruff-and-ready remix featuring none other than Puff Daddy!  Hear  today\\'s top hitmaker trade lines with the Blastmaster!!Also, an  experimental thrash cut, &quot;Just To Prove A Point&quot;, does just that.  KRS can pretty much rock over any style that he chooses! (and personally,  I would absolutely LOVE for him to do a whole Rock album!!!!)\", \"overall\": 3.0, \"summary\": \"A Solid Effort from the god of Hip-Hop!!\", \"unixReviewTime\": 922147200, \"reviewTime\": \"03 23, 1999\"}\\n', '{\"reviewerID\": \"A3G02Y0UWNWNE5\", \"asin\": \"B00000055E\", \"reviewerName\": \"D. Lee\", \"helpful\": [1, 1], \"reviewText\": \"This album has a lot of gems and hidden jewels, showcasing some of Pac\\'s greatest material.  It is obvious that 2pac was in a zone when he was making these songs.  You can just feel his energy as he raps non stop from verse to hook to verse to hook as he does on most of the songs on this album.  This collection was put together after 2pac died so it\\'s more of a collection of songs than an album per se, but there was something unique about Pac\\'s spirit when making most of these songs that provides a cohesive force of sorts that makes it all fit together well.  It seems as if Pac was just hitting his stride and really getting comfortable in the studio during these sessions, and the results are songs that sound really fresh and honest even when they are not his greatest material.  The first nine tracks are outstanding and add a very critical element to the big picture of who 2pac was as an artist.  \\'Open fire\\' is just intense and relentless.  The title track is much more personal and comtemplative.  It resonates most strongly when 2pac feeling battered and persecuted says, \\\\\"now the whole world\\'s callin\\' me a killer, all I ever did, was try to reach the kids with the real(la)\\\\\".  And this does honestly seem to be quite true of 2pac\\'s intentions up until the point where he made the material on this album.  I never truly understood some of the criticism that he faced.  This is pretty much the tone of the next five songs and they are all some of the best songs that he has ever written, especially \\'Hell Razor\\'.  This is something that you just have to hear to appreciate.  2pac basically just blindly lets it go, honestly pleading and screaming his heart and soul out. \\\\\"I\\'m Gettin Money\\\\\" is another winner and \\\\\"Like to Kick It\\\\\" is also a pretty solid effort.  The next two songs sound like nothing more than studio exercise and should have been left off of the album.  The last track on the first disc is quite similar to \\\\\"I\\'m gettin money\\\\\" and is also quite powerful.  The second disc does not have any really bad songs but it is not as good as the first one because it only has a few songs that stand out as really great. The songs on the second disc that stand out most to me are \\\\\"Hold on be Strong\\\\\", \\\\\"Do for love\\\\\", \\\\\"Nothin but love\\\\\"--finding 2pac at his most compassionate and idealistic saying, \\\\\"I help an old lady across the street, the cost is free I can\\'t take what she offers me, this is how the world could be, this is how the world should be\\\\\"--\\\\\"I Wonder if Heaven Got a Ghetto\\\\\", \\\\\"When I Get Free II\\\\\" and, the haunting \\\\\"Only Fear of Death\\\\\".  This is not exactly 2pac\\'s best album but it is one of his most enjoyable to just ride out and listen to.  It also gives you a clearer and much more well rounded picture of who 2pac was than the solid but seriously overrated \\\\\"All eyez on me\\\\\".  So I would strongly recommend it to anyone who wants to see more of 2pac than what the mainstream is accustomed to.\", \"overall\": 3.0, \"summary\": \"One of my favorite 2pac albums!\", \"unixReviewTime\": 1092009600, \"reviewTime\": \"08 9, 2004\"}\\n', '{\"reviewerID\": \"A1YS5QST2JD5RQ\", \"asin\": \"B00000064E\", \"reviewerName\": \"Todd D. Alt \\\\\"6-stringer\\\\\"\", \"helpful\": [2, 22], \"reviewText\": \"This guy got passed me back in the day, even though I basically bought every album that came out back in the 60s and 70s. I was listening to sattelite radio and heard a cut from this one and I liked the moodish thing going on and the hint of drug influence in there somewhere, so I checked out a few websites on Nick Drake. His tragic story kind of hooked me so I bought this CD. Quite frankly I find this to be interesting, but not something I would recomend to everybody. The mood of depression stays so heavy that every song sounds the same and makes me feel like I am watching one of those pulse readouts in a hospital room, and it is a flatliner. I suppose that some people dig this mellowed out sort of \\\\\"I wanna leave this world stuff\\\\\" but I get enough of it quick. Drake\\'s guitar work was a bit unique in it\\'s time, but as many times as I try, I just can\\'t quite get with this program. It is good to listen to and then take a nap.\", \"overall\": 2.0, \"summary\": \"Nothing to Get Excited About\", \"unixReviewTime\": 1205884800, \"reviewTime\": \"03 19, 2008\"}\\n', '{\"reviewerID\": \"A14INICZSIGAAA\", \"asin\": \"B0000009OU\", \"reviewerName\": \"Count Istvan Telecky\", \"helpful\": [0, 1], \"reviewText\": \"Fun songs on this sophomore effort...\\'Pump It Up\\', \\'Lipstick Vogue\\', \\'Radio, Radio\\'.  EC rocks harder here than on his debut recording.  This solidly rounds out my top 5 standard Costello recordings.\", \"overall\": 3.0, \"summary\": \"More fun from \\'little\\' Elvis\", \"unixReviewTime\": 1186617600, \"reviewTime\": \"08 9, 2007\"}\\n', '{\"reviewerID\": \"A2DJYERRIH5C8T\", \"asin\": \"B000000D7C\", \"reviewerName\": \"john thomas\", \"helpful\": [0, 0], \"reviewText\": \"I bought this on record when it first came out in 1982.  The only Hank record I had was The New South which I hated.  But over time it has gotten better.  All the songs on this album were hits.  So I knew the songs already when I bought this.  Then after this I started buying everything he started coming out with.  Although I have not bought anything new by him since Wild Streak.  This is a very good cd and what a greatest hits should be.  Starting off with Family Tradition and Whiskey Bent And Hell Bound.  My two least favorite songs on here.  I love every other song.  And the original A  Country Boy Can Survive is my favorite Hank song.  And just as meaningful now as it was 30 plus !!!! years ago.  This is country music with an attitude sung by a person that if you dont want to know what he thinks.  Then dont ask?  A singer who should be proud of what he has done and the name he has made on his own.\", \"overall\": 3.0, \"summary\": \"very proud of his own name\", \"unixReviewTime\": 1372550400, \"reviewTime\": \"06 30, 2013\"}\\n', '{\"reviewerID\": \"A2DVFHG099GUGE\", \"asin\": \"B000000OMB\", \"reviewerName\": \"sauerkraut\", \"helpful\": [4, 5], \"reviewText\": \"This self-titled, full-length debut album from Asia was released in 1982.  Nine tracks are contained.  The material is in a majestical pop rock musical direction.  Altogether, I find the songwriting to be pleasing, the musicianship to be skillful, and the sound quality to be worthy.  I enjoy John Wetton\\'s distinctive, smooth, meat-and-potatoes vocal delivery; also, he handles the bass guitar playing.  There is an agreeable synthesis in regard to Steve Howe\\'s gratifying guitar work and the tasteful keyboard playing of Geoff Downes.  As for the harmony vocals, I find them to be remarkably rich.  The compositions that I consider to be favorites are \\\\\"Only Time Will Tell,\\\\\" \\\\\"Sole Survivor,\\\\\" and \\\\\"Here Comes the Feeling.\\\\\"  The nicely tailored, attractive \\\\\"Only Time Will Tell\\\\\" displays memorable, stately keyboard work from Downes, indelible, pretty guitar playing from Howe, and lush, charming vocal harmonies.  The rousing \\\\\"Sole Survivor\\\\\" exhibits a dynamic, unforgettable chorus, while the spirited \\\\\"Here Comes the Feeling\\\\\" presents a catchy principal keyboard line and sparkling solo from Downes and an energy-filled, captivating refrain.  I think that the rest of the songs are enjoyable, too.  The sprightly \\\\\"Heat of the Moment\\\\\" provides lavish, admirable harmony vocals.  The appealing, easygoing \\\\\"One Step Closer\\\\\" features a mellow, flowing chorus.  The energetic \\\\\"Time Again\\\\\" sports creative musical arrangements and impressive utilization of vocal harmonies.  The lively \\\\\"Wildest Dreams\\\\\" supplies talented drumming from Carl Palmer.  The plaintive \\\\\"Without You\\\\\" furnishes somber, pleasant harmony vocals.  The active \\\\\"Cutting It Fine\\\\\" has a touching, regal ending keyboard passage from Downes.  The CD insert does not include the song lyrics.  The album cover artwork is interesting--a portion of this illustration is also displayed on the back of the CD jewel case.  The disc is just over 44 minutes.  The enthusiasm that emanates from this album is infectious.  Actually, I rate this piece of work 3.5 stars; it is a treat to listen to.\", \"overall\": 3.0, \"summary\": \"An impressive album\", \"unixReviewTime\": 1102118400, \"reviewTime\": \"12 4, 2004\"}\\n', '{\"reviewerID\": \"A3M362J5627PZV\", \"asin\": \"B000000OME\", \"reviewerName\": \"chensin\", \"helpful\": [7, 9], \"reviewText\": \"It was plain that JM wanted WTRF to reverse the tide of popular opinions about her music up to that point. Despite her famous aversion to touring, she commenced a world tour to promote this album in 82. The negative  reception of \\'Mingus\\' was plaguing her mind.Even in a bind, JM wouldnt  capitulate to conventional tastes, she couldnt and didnt. However, in an  album featuring guset vocals by the then pop king Lional Ritchie, and a  half-hearted attempt at reinterpreting a minor rock and roll classic  (\\'You\\'re so Square\\' by Elvis Presley), one senses a subtle wavering in JM\\'s  ideals.There are tunes of real beauty. \\'Chinese Cafe/Unchained Melody\\'  is fabled to be written after a meeting between JM and Carole King after  years of separation. The last track I Corinthian is an idealistic and  affecting look at love from the point of view of the gospel.Newcomers to  JM\\'s music to look elsewhere. This doeant represent JM\\'s best.\", \"overall\": 3.0, \"summary\": \"popular?\", \"unixReviewTime\": 956275200, \"reviewTime\": \"04 21, 2000\"}\\n', '{\"reviewerID\": \"A2HWD9PTM7RBXN\", \"asin\": \"B000000OPC\", \"reviewerName\": \"G. J Wiener\", \"helpful\": [0, 1], \"reviewText\": \"I respect Don Henley for creating a sound way different from the Eagles light country-rock sound.  His lyrical style is very thought provoking.  However, there are almost as many clunkers in this collection as there are  quality songs.  Try to get the cassette version at a bargain basement  price.\", \"overall\": 3.0, \"summary\": \"Some nice tracks but nothing special\", \"unixReviewTime\": 920073600, \"reviewTime\": \"02 27, 1999\"}\\n', '{\"reviewerID\": \"A2SPI5WNZLOJ2U\", \"asin\": \"B000000OPC\", \"reviewerName\": \"Sal Nudo\", \"helpful\": [1, 1], \"reviewText\": \"In the grand scheme of Don Henley\\'s career, \\\\\"Building the Perfect Beast\\\\\" is kind of a blip on the radar, not landmark material but also an interesting, worthy snapshot of where he was at the time. With the inclusion of a few synth-heavy songs and an overtly \\'80s pop sound on some of the tracks, it\\'s evident he was trying to distance himself from that little band he was in called the Eagles. One of the elements that make \\\\\"Beast\\\\\" a noteworthy album is the surprising diversity of the tunes. From ready-made-for-radio pop (\\\\\"All She Wants to do is Dance\\\\\") to somewhat contrived country (\\\\\"You\\'re Not Drinking Enough\\\\\") to the mechanically driven song \\\\\"Building the Perfect Beast,\\\\\" Henley and his fellow musicians crafted solid tunes on this 1984 album. Even the non-hits force you to sing along.Also to Henley\\'s credit, two of the hit songs, \\\\\"The Boys of Summer\\\\\" and \\\\\"Sunset Grill,\\\\\" contain an atmospheric edge to them that has helped both songs age well, become classics, really. Even when Henley laboriously stands on his soap box, as on \\\\\"A Month of Sundays,\\\\\" you can\\'t help but be impressed with his clear-eyed sound and vision, honesty and insightful lyrics.\", \"overall\": 3.0, \"summary\": \"Not a beast, but it is a good record\", \"unixReviewTime\": 1317772800, \"reviewTime\": \"10 5, 2011\"}\\n', '{\"reviewerID\": \"A2DVFHG099GUGE\", \"asin\": \"B000000OPZ\", \"reviewerName\": \"sauerkraut\", \"helpful\": [1, 1], \"reviewText\": \"There\\'s some pretty cool material on this album from Whitesnake.  A total of nine tracks is included, and the album is in a straight-ahead hard rock musical direction.  The musicianship, songwriting, and production are tight and satisfying.  Most of the songs are straight-ahead hard rockers.  John Sykes does a great job with the guitar playing.  His guitar riffs and solos are really good, and he also helped write almost all of the album\\'s material.  David Coverdale does a good job with the vocal duties.  There\\'s also some nice keyboard playing throughout.  The songs that I like the most are &quot;Still of the Night,&quot; the ballad &quot;Is This Love,&quot; and &quot;Don\\'t Turn Away.&quot;  All three are memorable and well-written songs.  The rest of the tunes are good, too.  This album is solid and worth a listen.\", \"overall\": 3.0, \"summary\": \"Energetic and  solid.\", \"unixReviewTime\": 973814400, \"reviewTime\": \"11 10, 2000\"}\\n', '{\"reviewerID\": \"A24QC50BP3Q4YS\", \"asin\": \"B000000ORJ\", \"reviewerName\": \"Noah \\\\\"earthvolunteer\\\\\"\", \"helpful\": [5, 9], \"reviewText\": \"With &quot;Letter From Home&quot;, Metheny and his Group take the listener on another familiar journey...one which began dramatically with &quot;First Circle&quot; and continued enthusiastically but less effectively with &quot;Still Life (Talking)&quot;.  Now comes a third installment in this particular style of fusion (with vocal) sound which the group launched in 1984 with &quot;First Circle&quot; and unfortunately it is beginning to sound a bit lackluster.The arrangements and the performances are all impeccable and beautifully recorded...there is just so little which is different or new here...why not go back and listen to a fantastic adventure of a recording called &quot;First Circle&quot;...when all of these musical ideas were just coming to light?Each of these previously-mentioned Metheny works through the years has a track or two, of course, which are truly emotional experiences and which transcend the mediocre.  With &quot;Offramp&quot; there was &quot;Are You Going With Me?&quot;...with &quot;First Circle&quot; there was the title track...with &quot;Still Life (Talking)&quot; there was &quot;Last Train Home.&quot;  With &quot;Letter From Home&quot; the track to marvel at is &quot;Dream of the Return&quot;...a timeless and climactic arrangement of guitars, keyboards and incomparable vocal work...it is amazing to listen to again and again.  There are many other melodious moments on &quot;Letter from Home&quot; as well...but the work as a whole is tried-and-true and not exceptional.\", \"overall\": 3.0, \"summary\": \"MORE SOLID COMPOSING &amp; PLAYING...BUT JUST MORE OF THE SAME.\", \"unixReviewTime\": 1036972800, \"reviewTime\": \"11 11, 2002\"}\\n', '{\"reviewerID\": \"A3K24XG2HBT7XU\", \"asin\": \"B000000OVC\", \"reviewerName\": \"Ray Bellotti\", \"helpful\": [0, 0], \"reviewText\": \"White Zombie remix.  Great song selection. Bad technically-sounds like it wa recorded in a coke can.  I\\'d rather listen to my full albums.\", \"overall\": 3.0, \"summary\": \"\\'90s metal\", \"unixReviewTime\": 1383436800, \"reviewTime\": \"11 3, 2013\"}\\n', '{\"reviewerID\": \"AFEN54UGJQOK9\", \"asin\": \"B000000OVC\", \"reviewerName\": \"wally gator \\\\\"the only\\\\\"\", \"helpful\": [1, 6], \"reviewText\": \"Astro creep is absolutely incredible. Zombie likes to mess around with different sounds. I got this when it came out cuz Astro Creep was so good. Its a little bit interesting, mainly for the chick on the cover.\", \"overall\": 3.0, \"summary\": \"I WOULD JUST STICK TO ASTRO CREEP\", \"unixReviewTime\": 1128988800, \"reviewTime\": \"10 11, 2005\"}\\n', '{\"reviewerID\": \"A1HBI9BBQIG1NH\", \"asin\": \"B000000OYK\", \"reviewerName\": \"Ward J. Lamb\", \"helpful\": [2, 6], \"reviewText\": \"This cd heralds a few choice cuts. They include &quot;We All Sleep  Alone&quot;, and &quot;Dangerous Times&quot;. Here Cher is moving and  dramatic. Her second two lps on Geffen were superior though, and &quot;Skin  Deep&quot; and &quot;Working Girl&quot;, are compositions that should never  have been selected. Cher has improved as a singer since this cd.Some will  love it for the hit &quot; I Found Someone&quot;, penned by Michael Bolton,  origional version by Laura Branigan. Cher cut the hit though.Not a bad  listen but Cher can be better.Chers first version of &quot;Bang Bang&quot;,  is much better in its\\' Sonny Bono Harold Battiste production. This one is  garish and hard.\", \"overall\": 3.0, \"summary\": \"This was Chers\\' re-entry into music after 5 yrs\", \"unixReviewTime\": 905040000, \"reviewTime\": \"09 6, 1998\"}\\n', '{\"reviewerID\": \"A1OB5L3WMHJAD9\", \"asin\": \"B000000W6X\", \"reviewerName\": \"Christopher Culver\", \"helpful\": [6, 10], \"reviewText\": \"Massive Attack\\'s first album, BLUE LINES, was an exciting and novel release that offered a range of superb tracks that didn\\'t let the listener down. Four years later, Massive Attack released PROTECTION, and this trip-hop band tripped up a little.PROTECTION nowhere offers the consistency that BLUE LINES did, and many of the songs are rather weak, especially the title track and &quot;Three.&quot; The last track, a live performance of The Doors\\' &quot;Light My Fire&quot;, seems pretty out-of-place.However, there are a couple of great tracks which stop this from being a bad album, the ganja-influenced &quot;Eurochild&quot; and &quot;Karmacoma,&quot; and the ambient &quot;Heat Miser.&quot; This songs make the album listenable, but somewhat disappointing all in all.Happily, Massive Attack\\'s next release, 1998\\'s MEZZANINE, is one of the best rock albums ever, and regains the perfection (surpasses it, in fact) of BLUE LINES.\", \"overall\": 3.0, \"summary\": \"Trip-hop trips up, a little\", \"unixReviewTime\": 979862400, \"reviewTime\": \"01 19, 2001\"}\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twnh1xiLkVzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_file = open('../categoryName_100_pos.txt', 'w+')\n",
        "for line in positives:\n",
        "  positive_file.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_EycqV5lCdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negative_file = open('../categoryName_100_neg.txt', 'w+')\n",
        "for line in negatives:\n",
        "  negative_file.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khfG58m2PpIV",
        "colab_type": "text"
      },
      "source": [
        "## Task 4\n",
        "\n",
        "Generate listing of false positives.\n",
        "Analyse the data from Amazon.\n",
        "What would be a way to utilize this data in order to generate larger lists of false positives?\n",
        "Derive a method that will allow you to predict over the full content of the file and create lists of:\n",
        "* True positive 'positive' predictions\n",
        "* False positive 'positive' predictions\n",
        "* True positive 'negative' predictions\n",
        "* False positive 'negative' predictions\n",
        "\n",
        "Save the four sets four your group submission:\n",
        "\n",
        "* categoryName_pos_tp.txt\n",
        "* categoryName_pos_fp.txt\n",
        "* categoryName_neg_tp.txt\n",
        "* categoryName_neg_fp.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDD6E7I2PpIZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e654d1ee-4f18-4923-dd7d-a1218689e01d"
      },
      "source": [
        "test_file = open('../reviews_Digital_Music_5.json', 'r')\n",
        "lines = test_file.readlines()\n",
        "pos_tp = []\n",
        "pos_fp = []\n",
        "neg_tp = []\n",
        "neg_fp = []\n",
        "for line in lines:\n",
        "  j_obj = json.loads(line)\n",
        "  logits = predictor.predict(j_obj['reviewText'])['logits']\n",
        "  label_id = np.argmax(logits)\n",
        "  prediction = model.vocab.get_token_from_index(label_id, 'labels')\n",
        "  if prediction == '4':\n",
        "    if j_obj['overall'] > 3:\n",
        "      pos_tp.append(line)\n",
        "      print(\"POS TP : \"+ str(len(pos_tp)))\n",
        "    else:\n",
        "      pos_fp.append(line)\n",
        "      print(\"POS FP : \"+ str(len(pos_fp)))\n",
        "  elif prediction == '0':\n",
        "    if j_obj['overall'] <= 3:\n",
        "      neg_tp.append(line)\n",
        "      print(\"NEG TP : \"+ str(len(neg_tp)))\n",
        "    else:\n",
        "      neg_fp.append(line)\n",
        "      print(\"NEG FP : \"+ str(len(neg_fp)))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "POS TP : 1\n",
            "POS TP : 2\n",
            "POS TP : 3\n",
            "POS FP : 1\n",
            "NEG FP : 1\n",
            "POS TP : 4\n",
            "POS TP : 5\n",
            "NEG FP : 2\n",
            "POS TP : 6\n",
            "NEG FP : 3\n",
            "POS TP : 7\n",
            "POS TP : 8\n",
            "POS TP : 9\n",
            "NEG FP : 4\n",
            "NEG FP : 5\n",
            "NEG TP : 1\n",
            "POS TP : 10\n",
            "NEG FP : 6\n",
            "POS TP : 11\n",
            "POS TP : 12\n",
            "POS TP : 13\n",
            "NEG FP : 7\n",
            "NEG FP : 8\n",
            "POS TP : 14\n",
            "NEG TP : 2\n",
            "POS TP : 15\n",
            "NEG FP : 9\n",
            "POS TP : 16\n",
            "NEG FP : 10\n",
            "NEG FP : 11\n",
            "NEG FP : 12\n",
            "POS TP : 17\n",
            "NEG FP : 13\n",
            "NEG FP : 14\n",
            "POS TP : 18\n",
            "POS TP : 19\n",
            "POS TP : 20\n",
            "NEG FP : 15\n",
            "NEG FP : 16\n",
            "NEG TP : 3\n",
            "NEG TP : 4\n",
            "NEG FP : 17\n",
            "POS TP : 21\n",
            "POS TP : 22\n",
            "NEG FP : 18\n",
            "NEG FP : 19\n",
            "POS TP : 23\n",
            "NEG FP : 20\n",
            "NEG FP : 21\n",
            "POS TP : 24\n",
            "POS TP : 25\n",
            "NEG TP : 5\n",
            "POS TP : 26\n",
            "NEG FP : 22\n",
            "POS TP : 27\n",
            "NEG FP : 23\n",
            "POS TP : 28\n",
            "POS TP : 29\n",
            "POS TP : 30\n",
            "NEG FP : 24\n",
            "POS TP : 31\n",
            "POS TP : 32\n",
            "POS TP : 33\n",
            "POS TP : 34\n",
            "POS TP : 35\n",
            "NEG TP : 6\n",
            "POS TP : 36\n",
            "NEG FP : 25\n",
            "POS TP : 37\n",
            "POS TP : 38\n",
            "NEG FP : 26\n",
            "POS TP : 39\n",
            "POS TP : 40\n",
            "NEG FP : 27\n",
            "NEG TP : 7\n",
            "POS TP : 41\n",
            "POS TP : 42\n",
            "POS TP : 43\n",
            "NEG FP : 28\n",
            "POS TP : 44\n",
            "POS TP : 45\n",
            "NEG FP : 29\n",
            "POS TP : 46\n",
            "NEG FP : 30\n",
            "POS TP : 47\n",
            "POS TP : 48\n",
            "POS TP : 49\n",
            "POS TP : 50\n",
            "POS TP : 51\n",
            "NEG FP : 31\n",
            "NEG FP : 32\n",
            "NEG FP : 33\n",
            "POS TP : 52\n",
            "POS TP : 53\n",
            "POS TP : 54\n",
            "POS TP : 55\n",
            "NEG FP : 34\n",
            "POS TP : 56\n",
            "POS TP : 57\n",
            "POS TP : 58\n",
            "POS TP : 59\n",
            "POS TP : 60\n",
            "POS TP : 61\n",
            "POS TP : 62\n",
            "POS TP : 63\n",
            "POS TP : 64\n",
            "POS TP : 65\n",
            "NEG FP : 35\n",
            "POS TP : 66\n",
            "NEG FP : 36\n",
            "POS TP : 67\n",
            "NEG FP : 37\n",
            "POS TP : 68\n",
            "POS TP : 69\n",
            "NEG FP : 38\n",
            "POS TP : 70\n",
            "NEG FP : 39\n",
            "POS TP : 71\n",
            "POS TP : 72\n",
            "POS TP : 73\n",
            "POS TP : 74\n",
            "NEG FP : 40\n",
            "POS TP : 75\n",
            "POS TP : 76\n",
            "POS FP : 2\n",
            "NEG TP : 8\n",
            "NEG FP : 41\n",
            "NEG TP : 9\n",
            "POS TP : 77\n",
            "POS TP : 78\n",
            "NEG FP : 42\n",
            "NEG FP : 43\n",
            "POS FP : 3\n",
            "POS TP : 79\n",
            "NEG FP : 44\n",
            "NEG FP : 45\n",
            "NEG FP : 46\n",
            "POS TP : 80\n",
            "NEG FP : 47\n",
            "NEG FP : 48\n",
            "POS TP : 81\n",
            "NEG FP : 49\n",
            "POS TP : 82\n",
            "POS TP : 83\n",
            "NEG FP : 50\n",
            "NEG FP : 51\n",
            "POS TP : 84\n",
            "NEG FP : 52\n",
            "NEG FP : 53\n",
            "NEG TP : 10\n",
            "NEG FP : 54\n",
            "NEG FP : 55\n",
            "NEG FP : 56\n",
            "NEG FP : 57\n",
            "NEG TP : 11\n",
            "POS TP : 85\n",
            "NEG FP : 58\n",
            "POS TP : 86\n",
            "NEG TP : 12\n",
            "NEG FP : 59\n",
            "NEG FP : 60\n",
            "POS TP : 87\n",
            "NEG FP : 61\n",
            "NEG TP : 13\n",
            "POS TP : 88\n",
            "NEG FP : 62\n",
            "NEG FP : 63\n",
            "NEG FP : 64\n",
            "NEG FP : 65\n",
            "NEG FP : 66\n",
            "NEG FP : 67\n",
            "NEG FP : 68\n",
            "POS TP : 89\n",
            "POS TP : 90\n",
            "POS TP : 91\n",
            "POS TP : 92\n",
            "NEG FP : 69\n",
            "NEG FP : 70\n",
            "NEG FP : 71\n",
            "POS TP : 93\n",
            "NEG FP : 72\n",
            "NEG FP : 73\n",
            "NEG FP : 74\n",
            "NEG FP : 75\n",
            "NEG FP : 76\n",
            "POS TP : 94\n",
            "POS TP : 95\n",
            "NEG FP : 77\n",
            "NEG FP : 78\n",
            "POS TP : 96\n",
            "NEG FP : 79\n",
            "POS TP : 97\n",
            "NEG FP : 80\n",
            "NEG FP : 81\n",
            "NEG FP : 82\n",
            "NEG FP : 83\n",
            "NEG FP : 84\n",
            "POS TP : 98\n",
            "POS TP : 99\n",
            "NEG FP : 85\n",
            "NEG FP : 86\n",
            "NEG FP : 87\n",
            "POS TP : 100\n",
            "POS TP : 101\n",
            "NEG FP : 88\n",
            "NEG FP : 89\n",
            "NEG FP : 90\n",
            "NEG FP : 91\n",
            "NEG FP : 92\n",
            "POS TP : 102\n",
            "NEG FP : 93\n",
            "POS TP : 103\n",
            "NEG FP : 94\n",
            "NEG FP : 95\n",
            "POS TP : 104\n",
            "POS TP : 105\n",
            "NEG FP : 96\n",
            "NEG FP : 97\n",
            "NEG FP : 98\n",
            "NEG FP : 99\n",
            "POS TP : 106\n",
            "NEG FP : 100\n",
            "NEG FP : 101\n",
            "POS TP : 107\n",
            "NEG FP : 102\n",
            "NEG FP : 103\n",
            "POS TP : 108\n",
            "POS TP : 109\n",
            "NEG FP : 104\n",
            "NEG FP : 105\n",
            "NEG FP : 106\n",
            "POS TP : 110\n",
            "NEG FP : 107\n",
            "NEG FP : 108\n",
            "POS TP : 111\n",
            "POS TP : 112\n",
            "POS TP : 113\n",
            "NEG FP : 109\n",
            "NEG FP : 110\n",
            "NEG FP : 111\n",
            "NEG FP : 112\n",
            "NEG FP : 113\n",
            "POS TP : 114\n",
            "NEG FP : 114\n",
            "POS TP : 115\n",
            "NEG FP : 115\n",
            "NEG FP : 116\n",
            "NEG FP : 117\n",
            "NEG FP : 118\n",
            "POS TP : 116\n",
            "NEG FP : 119\n",
            "POS TP : 117\n",
            "POS TP : 118\n",
            "NEG FP : 120\n",
            "POS TP : 119\n",
            "NEG FP : 121\n",
            "NEG FP : 122\n",
            "NEG FP : 123\n",
            "POS TP : 120\n",
            "NEG FP : 124\n",
            "NEG FP : 125\n",
            "POS TP : 121\n",
            "NEG FP : 126\n",
            "NEG FP : 127\n",
            "NEG FP : 128\n",
            "POS TP : 122\n",
            "NEG FP : 129\n",
            "NEG FP : 130\n",
            "POS TP : 123\n",
            "NEG FP : 131\n",
            "POS TP : 124\n",
            "POS TP : 125\n",
            "POS TP : 126\n",
            "NEG FP : 132\n",
            "NEG FP : 133\n",
            "NEG FP : 134\n",
            "NEG TP : 14\n",
            "NEG FP : 135\n",
            "NEG TP : 15\n",
            "POS FP : 4\n",
            "POS TP : 127\n",
            "NEG FP : 136\n",
            "NEG TP : 16\n",
            "NEG TP : 17\n",
            "POS TP : 128\n",
            "POS TP : 129\n",
            "NEG TP : 18\n",
            "POS TP : 130\n",
            "POS TP : 131\n",
            "NEG FP : 137\n",
            "NEG TP : 19\n",
            "NEG FP : 138\n",
            "NEG FP : 139\n",
            "POS TP : 132\n",
            "NEG FP : 140\n",
            "POS TP : 133\n",
            "NEG FP : 141\n",
            "POS TP : 134\n",
            "NEG FP : 142\n",
            "POS TP : 135\n",
            "POS TP : 136\n",
            "NEG FP : 143\n",
            "POS TP : 137\n",
            "NEG FP : 144\n",
            "POS TP : 138\n",
            "NEG FP : 145\n",
            "NEG FP : 146\n",
            "NEG TP : 20\n",
            "NEG FP : 147\n",
            "NEG FP : 148\n",
            "POS TP : 139\n",
            "NEG FP : 149\n",
            "POS FP : 5\n",
            "NEG FP : 150\n",
            "NEG FP : 151\n",
            "NEG TP : 21\n",
            "NEG FP : 152\n",
            "NEG FP : 153\n",
            "NEG FP : 154\n",
            "NEG TP : 22\n",
            "POS TP : 140\n",
            "NEG FP : 155\n",
            "NEG FP : 156\n",
            "POS FP : 6\n",
            "NEG FP : 157\n",
            "NEG FP : 158\n",
            "POS TP : 141\n",
            "NEG FP : 159\n",
            "NEG FP : 160\n",
            "NEG FP : 161\n",
            "NEG FP : 162\n",
            "NEG FP : 163\n",
            "NEG TP : 23\n",
            "POS TP : 142\n",
            "NEG TP : 24\n",
            "POS TP : 143\n",
            "POS TP : 144\n",
            "NEG FP : 164\n",
            "POS TP : 145\n",
            "POS TP : 146\n",
            "POS TP : 147\n",
            "POS TP : 148\n",
            "POS TP : 149\n",
            "POS TP : 150\n",
            "NEG FP : 165\n",
            "NEG FP : 166\n",
            "NEG FP : 167\n",
            "NEG FP : 168\n",
            "POS TP : 151\n",
            "NEG FP : 169\n",
            "POS FP : 7\n",
            "NEG TP : 25\n",
            "POS TP : 152\n",
            "NEG TP : 26\n",
            "POS TP : 153\n",
            "POS TP : 154\n",
            "NEG FP : 170\n",
            "NEG FP : 171\n",
            "POS TP : 155\n",
            "POS TP : 156\n",
            "NEG TP : 27\n",
            "POS TP : 157\n",
            "NEG FP : 172\n",
            "POS TP : 158\n",
            "POS TP : 159\n",
            "NEG FP : 173\n",
            "POS TP : 160\n",
            "POS TP : 161\n",
            "NEG FP : 174\n",
            "POS TP : 162\n",
            "NEG FP : 175\n",
            "POS TP : 163\n",
            "POS TP : 164\n",
            "POS TP : 165\n",
            "NEG FP : 176\n",
            "POS TP : 166\n",
            "NEG FP : 177\n",
            "NEG FP : 178\n",
            "NEG FP : 179\n",
            "NEG FP : 180\n",
            "POS TP : 167\n",
            "NEG FP : 181\n",
            "NEG FP : 182\n",
            "POS TP : 168\n",
            "NEG FP : 183\n",
            "POS TP : 169\n",
            "POS TP : 170\n",
            "NEG FP : 184\n",
            "POS TP : 171\n",
            "POS TP : 172\n",
            "NEG FP : 185\n",
            "POS TP : 173\n",
            "POS TP : 174\n",
            "POS TP : 175\n",
            "NEG FP : 186\n",
            "NEG TP : 28\n",
            "POS TP : 176\n",
            "POS TP : 177\n",
            "NEG FP : 187\n",
            "NEG FP : 188\n",
            "POS TP : 178\n",
            "POS TP : 179\n",
            "NEG FP : 189\n",
            "POS TP : 180\n",
            "POS TP : 181\n",
            "NEG FP : 190\n",
            "NEG TP : 29\n",
            "POS TP : 182\n",
            "POS TP : 183\n",
            "NEG TP : 30\n",
            "POS TP : 184\n",
            "POS TP : 185\n",
            "POS TP : 186\n",
            "NEG FP : 191\n",
            "NEG FP : 192\n",
            "POS FP : 8\n",
            "NEG FP : 193\n",
            "NEG FP : 194\n",
            "POS TP : 187\n",
            "NEG FP : 195\n",
            "NEG FP : 196\n",
            "NEG FP : 197\n",
            "POS TP : 188\n",
            "NEG FP : 198\n",
            "POS TP : 189\n",
            "NEG FP : 199\n",
            "NEG FP : 200\n",
            "NEG FP : 201\n",
            "NEG FP : 202\n",
            "NEG FP : 203\n",
            "NEG FP : 204\n",
            "POS TP : 190\n",
            "POS TP : 191\n",
            "NEG FP : 205\n",
            "POS TP : 192\n",
            "POS TP : 193\n",
            "POS TP : 194\n",
            "POS TP : 195\n",
            "NEG FP : 206\n",
            "POS TP : 196\n",
            "NEG FP : 207\n",
            "POS TP : 197\n",
            "POS TP : 198\n",
            "POS TP : 199\n",
            "NEG FP : 208\n",
            "NEG FP : 209\n",
            "POS TP : 200\n",
            "POS TP : 201\n",
            "NEG TP : 31\n",
            "POS TP : 202\n",
            "NEG TP : 32\n",
            "NEG FP : 210\n",
            "POS TP : 203\n",
            "POS TP : 204\n",
            "POS TP : 205\n",
            "POS TP : 206\n",
            "POS TP : 207\n",
            "NEG FP : 211\n",
            "POS TP : 208\n",
            "POS TP : 209\n",
            "NEG FP : 212\n",
            "NEG FP : 213\n",
            "NEG TP : 33\n",
            "NEG TP : 34\n",
            "NEG FP : 214\n",
            "NEG FP : 215\n",
            "POS TP : 210\n",
            "NEG FP : 216\n",
            "NEG FP : 217\n",
            "NEG FP : 218\n",
            "POS TP : 211\n",
            "NEG FP : 219\n",
            "POS TP : 212\n",
            "POS TP : 213\n",
            "NEG FP : 220\n",
            "NEG FP : 221\n",
            "POS TP : 214\n",
            "POS TP : 215\n",
            "NEG TP : 35\n",
            "POS TP : 216\n",
            "POS FP : 9\n",
            "NEG FP : 222\n",
            "POS TP : 217\n",
            "NEG FP : 223\n",
            "NEG TP : 36\n",
            "POS TP : 218\n",
            "POS TP : 219\n",
            "POS TP : 220\n",
            "NEG FP : 224\n",
            "NEG FP : 225\n",
            "POS TP : 221\n",
            "NEG FP : 226\n",
            "POS TP : 222\n",
            "POS TP : 223\n",
            "NEG FP : 227\n",
            "NEG FP : 228\n",
            "POS TP : 224\n",
            "NEG FP : 229\n",
            "NEG TP : 37\n",
            "POS TP : 225\n",
            "POS TP : 226\n",
            "POS TP : 227\n",
            "NEG TP : 38\n",
            "POS TP : 228\n",
            "NEG FP : 230\n",
            "POS TP : 229\n",
            "NEG FP : 231\n",
            "NEG TP : 39\n",
            "NEG TP : 40\n",
            "POS TP : 230\n",
            "NEG TP : 41\n",
            "NEG FP : 232\n",
            "NEG FP : 233\n",
            "POS TP : 231\n",
            "POS TP : 232\n",
            "POS TP : 233\n",
            "NEG FP : 234\n",
            "NEG FP : 235\n",
            "NEG FP : 236\n",
            "POS TP : 234\n",
            "POS TP : 235\n",
            "POS TP : 236\n",
            "POS TP : 237\n",
            "POS TP : 238\n",
            "NEG TP : 42\n",
            "POS TP : 239\n",
            "POS TP : 240\n",
            "POS TP : 241\n",
            "POS TP : 242\n",
            "POS TP : 243\n",
            "POS TP : 244\n",
            "NEG FP : 237\n",
            "POS TP : 245\n",
            "NEG FP : 238\n",
            "POS TP : 246\n",
            "POS TP : 247\n",
            "POS TP : 248\n",
            "NEG FP : 239\n",
            "NEG FP : 240\n",
            "POS TP : 249\n",
            "POS TP : 250\n",
            "NEG FP : 241\n",
            "POS TP : 251\n",
            "POS TP : 252\n",
            "NEG FP : 242\n",
            "POS TP : 253\n",
            "POS TP : 254\n",
            "POS TP : 255\n",
            "NEG FP : 243\n",
            "POS TP : 256\n",
            "NEG FP : 244\n",
            "POS TP : 257\n",
            "POS TP : 258\n",
            "NEG FP : 245\n",
            "NEG FP : 246\n",
            "POS TP : 259\n",
            "NEG FP : 247\n",
            "POS TP : 260\n",
            "NEG FP : 248\n",
            "POS TP : 261\n",
            "NEG FP : 249\n",
            "POS TP : 262\n",
            "NEG FP : 250\n",
            "POS TP : 263\n",
            "POS TP : 264\n",
            "NEG FP : 251\n",
            "POS TP : 265\n",
            "POS TP : 266\n",
            "NEG TP : 43\n",
            "NEG FP : 252\n",
            "NEG FP : 253\n",
            "NEG TP : 44\n",
            "POS TP : 267\n",
            "POS TP : 268\n",
            "NEG TP : 45\n",
            "NEG TP : 46\n",
            "POS TP : 269\n",
            "POS TP : 270\n",
            "NEG FP : 254\n",
            "NEG FP : 255\n",
            "NEG FP : 256\n",
            "POS TP : 271\n",
            "POS TP : 272\n",
            "POS TP : 273\n",
            "POS TP : 274\n",
            "POS TP : 275\n",
            "NEG FP : 257\n",
            "POS FP : 10\n",
            "NEG FP : 258\n",
            "POS TP : 276\n",
            "NEG FP : 259\n",
            "NEG FP : 260\n",
            "NEG TP : 47\n",
            "NEG FP : 261\n",
            "POS TP : 277\n",
            "POS FP : 11\n",
            "POS TP : 278\n",
            "NEG FP : 262\n",
            "NEG FP : 263\n",
            "POS FP : 12\n",
            "NEG FP : 264\n",
            "POS TP : 279\n",
            "POS TP : 280\n",
            "POS TP : 281\n",
            "POS TP : 282\n",
            "POS TP : 283\n",
            "POS FP : 13\n",
            "NEG FP : 265\n",
            "POS TP : 284\n",
            "NEG TP : 48\n",
            "POS TP : 285\n",
            "NEG TP : 49\n",
            "POS TP : 286\n",
            "NEG FP : 266\n",
            "POS TP : 287\n",
            "POS TP : 288\n",
            "POS TP : 289\n",
            "NEG TP : 50\n",
            "POS FP : 14\n",
            "POS TP : 290\n",
            "POS TP : 291\n",
            "POS TP : 292\n",
            "NEG FP : 267\n",
            "NEG FP : 268\n",
            "POS TP : 293\n",
            "POS TP : 294\n",
            "NEG FP : 269\n",
            "POS TP : 295\n",
            "NEG FP : 270\n",
            "POS TP : 296\n",
            "POS TP : 297\n",
            "POS TP : 298\n",
            "POS TP : 299\n",
            "POS TP : 300\n",
            "NEG FP : 271\n",
            "POS TP : 301\n",
            "NEG FP : 272\n",
            "NEG TP : 51\n",
            "POS TP : 302\n",
            "POS TP : 303\n",
            "POS TP : 304\n",
            "POS TP : 305\n",
            "POS TP : 306\n",
            "NEG FP : 273\n",
            "NEG TP : 52\n",
            "POS TP : 307\n",
            "POS TP : 308\n",
            "POS TP : 309\n",
            "POS TP : 310\n",
            "POS TP : 311\n",
            "NEG FP : 274\n",
            "POS TP : 312\n",
            "POS TP : 313\n",
            "POS TP : 314\n",
            "POS TP : 315\n",
            "POS FP : 15\n",
            "POS TP : 316\n",
            "NEG FP : 275\n",
            "POS TP : 317\n",
            "POS TP : 318\n",
            "POS TP : 319\n",
            "POS TP : 320\n",
            "POS TP : 321\n",
            "POS TP : 322\n",
            "POS TP : 323\n",
            "NEG FP : 276\n",
            "POS TP : 324\n",
            "POS TP : 325\n",
            "NEG FP : 277\n",
            "POS TP : 326\n",
            "POS TP : 327\n",
            "POS TP : 328\n",
            "NEG FP : 278\n",
            "POS TP : 329\n",
            "NEG FP : 279\n",
            "POS TP : 330\n",
            "NEG FP : 280\n",
            "NEG TP : 53\n",
            "POS TP : 331\n",
            "POS TP : 332\n",
            "POS TP : 333\n",
            "POS TP : 334\n",
            "POS TP : 335\n",
            "NEG FP : 281\n",
            "NEG FP : 282\n",
            "NEG TP : 54\n",
            "POS TP : 336\n",
            "NEG TP : 55\n",
            "POS TP : 337\n",
            "NEG FP : 283\n",
            "NEG FP : 284\n",
            "NEG FP : 285\n",
            "POS TP : 338\n",
            "POS TP : 339\n",
            "NEG FP : 286\n",
            "NEG FP : 287\n",
            "NEG FP : 288\n",
            "NEG FP : 289\n",
            "POS TP : 340\n",
            "POS TP : 341\n",
            "POS TP : 342\n",
            "POS TP : 343\n",
            "POS TP : 344\n",
            "POS TP : 345\n",
            "POS TP : 346\n",
            "POS TP : 347\n",
            "POS TP : 348\n",
            "NEG FP : 290\n",
            "NEG FP : 291\n",
            "POS TP : 349\n",
            "POS TP : 350\n",
            "NEG FP : 292\n",
            "POS TP : 351\n",
            "NEG FP : 293\n",
            "NEG FP : 294\n",
            "POS TP : 352\n",
            "NEG FP : 295\n",
            "POS TP : 353\n",
            "NEG FP : 296\n",
            "NEG TP : 56\n",
            "POS TP : 354\n",
            "POS TP : 355\n",
            "NEG FP : 297\n",
            "POS TP : 356\n",
            "POS TP : 357\n",
            "POS TP : 358\n",
            "NEG FP : 298\n",
            "POS TP : 359\n",
            "NEG TP : 57\n",
            "POS TP : 360\n",
            "POS TP : 361\n",
            "NEG FP : 299\n",
            "NEG FP : 300\n",
            "POS TP : 362\n",
            "POS TP : 363\n",
            "NEG TP : 58\n",
            "POS TP : 364\n",
            "POS TP : 365\n",
            "NEG FP : 301\n",
            "POS FP : 16\n",
            "POS FP : 17\n",
            "POS TP : 366\n",
            "POS TP : 367\n",
            "POS TP : 368\n",
            "NEG FP : 302\n",
            "POS TP : 369\n",
            "POS TP : 370\n",
            "NEG TP : 59\n",
            "POS TP : 371\n",
            "NEG TP : 60\n",
            "POS TP : 372\n",
            "POS TP : 373\n",
            "NEG FP : 303\n",
            "NEG FP : 304\n",
            "NEG FP : 305\n",
            "POS TP : 374\n",
            "POS TP : 375\n",
            "POS TP : 376\n",
            "NEG FP : 306\n",
            "NEG FP : 307\n",
            "POS TP : 377\n",
            "POS FP : 18\n",
            "NEG TP : 61\n",
            "NEG FP : 308\n",
            "NEG FP : 309\n",
            "POS TP : 378\n",
            "NEG FP : 310\n",
            "POS TP : 379\n",
            "POS TP : 380\n",
            "NEG FP : 311\n",
            "NEG FP : 312\n",
            "POS TP : 381\n",
            "POS TP : 382\n",
            "NEG FP : 313\n",
            "NEG TP : 62\n",
            "POS TP : 383\n",
            "POS TP : 384\n",
            "POS TP : 385\n",
            "NEG FP : 314\n",
            "POS TP : 386\n",
            "POS TP : 387\n",
            "NEG FP : 315\n",
            "POS TP : 388\n",
            "POS TP : 389\n",
            "NEG FP : 316\n",
            "POS TP : 390\n",
            "NEG FP : 317\n",
            "POS TP : 391\n",
            "POS TP : 392\n",
            "POS TP : 393\n",
            "NEG FP : 318\n",
            "NEG FP : 319\n",
            "NEG FP : 320\n",
            "POS TP : 394\n",
            "POS TP : 395\n",
            "POS TP : 396\n",
            "NEG FP : 321\n",
            "POS FP : 19\n",
            "POS TP : 397\n",
            "POS TP : 398\n",
            "NEG FP : 322\n",
            "POS TP : 399\n",
            "NEG FP : 323\n",
            "NEG FP : 324\n",
            "POS TP : 400\n",
            "NEG TP : 63\n",
            "POS TP : 401\n",
            "NEG FP : 325\n",
            "POS TP : 402\n",
            "NEG FP : 326\n",
            "NEG FP : 327\n",
            "POS TP : 403\n",
            "NEG FP : 328\n",
            "NEG FP : 329\n",
            "POS TP : 404\n",
            "NEG FP : 330\n",
            "NEG FP : 331\n",
            "NEG FP : 332\n",
            "POS TP : 405\n",
            "POS TP : 406\n",
            "POS TP : 407\n",
            "NEG FP : 333\n",
            "NEG FP : 334\n",
            "POS TP : 408\n",
            "NEG FP : 335\n",
            "NEG FP : 336\n",
            "NEG TP : 64\n",
            "NEG TP : 65\n",
            "NEG TP : 66\n",
            "NEG FP : 337\n",
            "NEG FP : 338\n",
            "POS TP : 409\n",
            "NEG FP : 339\n",
            "NEG FP : 340\n",
            "NEG TP : 67\n",
            "NEG FP : 341\n",
            "NEG FP : 342\n",
            "NEG FP : 343\n",
            "POS TP : 410\n",
            "NEG FP : 344\n",
            "POS TP : 411\n",
            "POS TP : 412\n",
            "POS TP : 413\n",
            "NEG FP : 345\n",
            "POS TP : 414\n",
            "NEG FP : 346\n",
            "POS TP : 415\n",
            "POS TP : 416\n",
            "POS TP : 417\n",
            "NEG FP : 347\n",
            "POS TP : 418\n",
            "POS TP : 419\n",
            "POS TP : 420\n",
            "NEG FP : 348\n",
            "POS TP : 421\n",
            "POS TP : 422\n",
            "POS TP : 423\n",
            "POS TP : 424\n",
            "NEG FP : 349\n",
            "NEG FP : 350\n",
            "NEG FP : 351\n",
            "POS TP : 425\n",
            "POS TP : 426\n",
            "POS TP : 427\n",
            "POS TP : 428\n",
            "POS TP : 429\n",
            "POS TP : 430\n",
            "POS TP : 431\n",
            "NEG FP : 352\n",
            "POS TP : 432\n",
            "POS TP : 433\n",
            "NEG FP : 353\n",
            "POS TP : 434\n",
            "POS FP : 20\n",
            "POS TP : 435\n",
            "POS TP : 436\n",
            "NEG FP : 354\n",
            "NEG FP : 355\n",
            "NEG FP : 356\n",
            "NEG FP : 357\n",
            "POS TP : 437\n",
            "NEG FP : 358\n",
            "NEG FP : 359\n",
            "POS TP : 438\n",
            "POS TP : 439\n",
            "POS TP : 440\n",
            "POS TP : 441\n",
            "POS TP : 442\n",
            "NEG FP : 360\n",
            "POS TP : 443\n",
            "POS TP : 444\n",
            "POS TP : 445\n",
            "NEG FP : 361\n",
            "POS TP : 446\n",
            "POS TP : 447\n",
            "POS TP : 448\n",
            "POS TP : 449\n",
            "POS TP : 450\n",
            "POS TP : 451\n",
            "POS TP : 452\n",
            "POS TP : 453\n",
            "POS TP : 454\n",
            "NEG FP : 362\n",
            "POS TP : 455\n",
            "NEG FP : 363\n",
            "NEG FP : 364\n",
            "NEG FP : 365\n",
            "NEG FP : 366\n",
            "POS TP : 456\n",
            "POS TP : 457\n",
            "NEG FP : 367\n",
            "POS TP : 458\n",
            "POS TP : 459\n",
            "POS TP : 460\n",
            "POS TP : 461\n",
            "POS TP : 462\n",
            "POS TP : 463\n",
            "POS TP : 464\n",
            "POS TP : 465\n",
            "POS TP : 466\n",
            "POS TP : 467\n",
            "NEG FP : 368\n",
            "NEG FP : 369\n",
            "NEG FP : 370\n",
            "POS TP : 468\n",
            "POS TP : 469\n",
            "POS TP : 470\n",
            "NEG FP : 371\n",
            "POS TP : 471\n",
            "NEG FP : 372\n",
            "POS TP : 472\n",
            "POS TP : 473\n",
            "POS TP : 474\n",
            "NEG FP : 373\n",
            "POS TP : 475\n",
            "NEG FP : 374\n",
            "POS TP : 476\n",
            "NEG FP : 375\n",
            "POS TP : 477\n",
            "POS TP : 478\n",
            "NEG FP : 376\n",
            "POS TP : 479\n",
            "NEG FP : 377\n",
            "POS TP : 480\n",
            "POS TP : 481\n",
            "POS TP : 482\n",
            "POS TP : 483\n",
            "POS TP : 484\n",
            "POS TP : 485\n",
            "POS TP : 486\n",
            "NEG FP : 378\n",
            "POS TP : 487\n",
            "NEG FP : 379\n",
            "POS TP : 488\n",
            "POS TP : 489\n",
            "NEG TP : 68\n",
            "POS TP : 490\n",
            "POS TP : 491\n",
            "POS TP : 492\n",
            "POS TP : 493\n",
            "NEG FP : 380\n",
            "NEG FP : 381\n",
            "POS TP : 494\n",
            "NEG FP : 382\n",
            "NEG FP : 383\n",
            "POS FP : 21\n",
            "NEG FP : 384\n",
            "POS TP : 495\n",
            "POS TP : 496\n",
            "POS TP : 497\n",
            "POS TP : 498\n",
            "POS TP : 499\n",
            "NEG FP : 385\n",
            "POS TP : 500\n",
            "NEG FP : 386\n",
            "POS TP : 501\n",
            "NEG FP : 387\n",
            "POS TP : 502\n",
            "POS TP : 503\n",
            "POS TP : 504\n",
            "POS TP : 505\n",
            "NEG TP : 69\n",
            "POS TP : 506\n",
            "NEG FP : 388\n",
            "POS TP : 507\n",
            "POS TP : 508\n",
            "POS TP : 509\n",
            "NEG FP : 389\n",
            "NEG FP : 390\n",
            "POS TP : 510\n",
            "NEG FP : 391\n",
            "NEG TP : 70\n",
            "POS TP : 511\n",
            "POS TP : 512\n",
            "NEG FP : 392\n",
            "POS TP : 513\n",
            "POS TP : 514\n",
            "POS TP : 515\n",
            "POS TP : 516\n",
            "NEG FP : 393\n",
            "NEG FP : 394\n",
            "NEG FP : 395\n",
            "POS TP : 517\n",
            "NEG FP : 396\n",
            "POS TP : 518\n",
            "NEG FP : 397\n",
            "POS TP : 519\n",
            "POS TP : 520\n",
            "NEG FP : 398\n",
            "POS TP : 521\n",
            "POS TP : 522\n",
            "POS TP : 523\n",
            "POS TP : 524\n",
            "NEG FP : 399\n",
            "NEG FP : 400\n",
            "NEG FP : 401\n",
            "NEG FP : 402\n",
            "NEG FP : 403\n",
            "NEG FP : 404\n",
            "POS TP : 525\n",
            "NEG FP : 405\n",
            "NEG FP : 406\n",
            "POS TP : 526\n",
            "POS TP : 527\n",
            "POS TP : 528\n",
            "NEG FP : 407\n",
            "NEG FP : 408\n",
            "POS TP : 529\n",
            "NEG FP : 409\n",
            "NEG TP : 71\n",
            "NEG FP : 410\n",
            "POS TP : 530\n",
            "POS TP : 531\n",
            "NEG FP : 411\n",
            "POS TP : 532\n",
            "POS TP : 533\n",
            "POS TP : 534\n",
            "POS TP : 535\n",
            "NEG FP : 412\n",
            "NEG TP : 72\n",
            "POS TP : 536\n",
            "NEG FP : 413\n",
            "NEG FP : 414\n",
            "POS TP : 537\n",
            "NEG FP : 415\n",
            "NEG FP : 416\n",
            "NEG FP : 417\n",
            "NEG FP : 418\n",
            "POS FP : 22\n",
            "POS TP : 538\n",
            "NEG FP : 419\n",
            "POS TP : 539\n",
            "NEG FP : 420\n",
            "POS TP : 540\n",
            "POS TP : 541\n",
            "NEG TP : 73\n",
            "NEG FP : 421\n",
            "POS TP : 542\n",
            "POS TP : 543\n",
            "NEG FP : 422\n",
            "NEG FP : 423\n",
            "POS TP : 544\n",
            "POS TP : 545\n",
            "POS TP : 546\n",
            "NEG FP : 424\n",
            "NEG FP : 425\n",
            "NEG FP : 426\n",
            "NEG FP : 427\n",
            "NEG FP : 428\n",
            "NEG FP : 429\n",
            "POS TP : 547\n",
            "NEG FP : 430\n",
            "POS TP : 548\n",
            "NEG FP : 431\n",
            "POS TP : 549\n",
            "NEG FP : 432\n",
            "POS TP : 550\n",
            "NEG FP : 433\n",
            "POS TP : 551\n",
            "NEG FP : 434\n",
            "NEG FP : 435\n",
            "NEG FP : 436\n",
            "NEG FP : 437\n",
            "POS TP : 552\n",
            "NEG FP : 438\n",
            "NEG FP : 439\n",
            "POS TP : 553\n",
            "POS TP : 554\n",
            "NEG FP : 440\n",
            "NEG FP : 441\n",
            "POS FP : 23\n",
            "NEG FP : 442\n",
            "NEG FP : 443\n",
            "NEG FP : 444\n",
            "NEG FP : 445\n",
            "NEG FP : 446\n",
            "NEG FP : 447\n",
            "NEG FP : 448\n",
            "POS TP : 555\n",
            "POS TP : 556\n",
            "NEG FP : 449\n",
            "NEG FP : 450\n",
            "NEG FP : 451\n",
            "NEG FP : 452\n",
            "NEG TP : 74\n",
            "NEG FP : 453\n",
            "POS TP : 557\n",
            "POS TP : 558\n",
            "NEG FP : 454\n",
            "NEG FP : 455\n",
            "NEG FP : 456\n",
            "NEG FP : 457\n",
            "POS TP : 559\n",
            "POS TP : 560\n",
            "NEG FP : 458\n",
            "POS TP : 561\n",
            "NEG FP : 459\n",
            "NEG FP : 460\n",
            "NEG FP : 461\n",
            "NEG FP : 462\n",
            "NEG TP : 75\n",
            "NEG TP : 76\n",
            "POS TP : 562\n",
            "POS TP : 563\n",
            "POS TP : 564\n",
            "POS TP : 565\n",
            "NEG FP : 463\n",
            "POS TP : 566\n",
            "POS TP : 567\n",
            "NEG FP : 464\n",
            "NEG FP : 465\n",
            "NEG FP : 466\n",
            "POS TP : 568\n",
            "POS TP : 569\n",
            "NEG FP : 467\n",
            "POS TP : 570\n",
            "POS TP : 571\n",
            "POS TP : 572\n",
            "POS TP : 573\n",
            "NEG FP : 468\n",
            "POS TP : 574\n",
            "POS TP : 575\n",
            "POS TP : 576\n",
            "POS TP : 577\n",
            "POS TP : 578\n",
            "POS TP : 579\n",
            "POS TP : 580\n",
            "POS TP : 581\n",
            "NEG FP : 469\n",
            "POS TP : 582\n",
            "POS FP : 24\n",
            "NEG TP : 77\n",
            "NEG FP : 470\n",
            "NEG FP : 471\n",
            "NEG FP : 472\n",
            "POS FP : 25\n",
            "POS TP : 583\n",
            "NEG FP : 473\n",
            "POS TP : 584\n",
            "POS TP : 585\n",
            "POS TP : 586\n",
            "POS TP : 587\n",
            "NEG FP : 474\n",
            "POS TP : 588\n",
            "NEG FP : 475\n",
            "NEG FP : 476\n",
            "NEG FP : 477\n",
            "POS TP : 589\n",
            "NEG FP : 478\n",
            "POS TP : 590\n",
            "NEG FP : 479\n",
            "POS TP : 591\n",
            "POS TP : 592\n",
            "NEG FP : 480\n",
            "POS TP : 593\n",
            "POS TP : 594\n",
            "POS TP : 595\n",
            "POS TP : 596\n",
            "NEG FP : 481\n",
            "POS TP : 597\n",
            "NEG FP : 482\n",
            "NEG FP : 483\n",
            "POS TP : 598\n",
            "NEG FP : 484\n",
            "POS TP : 599\n",
            "POS TP : 600\n",
            "POS TP : 601\n",
            "POS TP : 602\n",
            "POS TP : 603\n",
            "POS TP : 604\n",
            "POS TP : 605\n",
            "POS FP : 26\n",
            "POS TP : 606\n",
            "POS TP : 607\n",
            "POS TP : 608\n",
            "NEG FP : 485\n",
            "POS TP : 609\n",
            "NEG FP : 486\n",
            "POS TP : 610\n",
            "NEG FP : 487\n",
            "POS TP : 611\n",
            "POS TP : 612\n",
            "POS TP : 613\n",
            "NEG FP : 488\n",
            "POS TP : 614\n",
            "POS TP : 615\n",
            "POS TP : 616\n",
            "POS FP : 27\n",
            "POS TP : 617\n",
            "POS TP : 618\n",
            "POS TP : 619\n",
            "POS TP : 620\n",
            "POS TP : 621\n",
            "POS TP : 622\n",
            "POS TP : 623\n",
            "POS TP : 624\n",
            "POS TP : 625\n",
            "POS TP : 626\n",
            "POS TP : 627\n",
            "NEG FP : 489\n",
            "POS TP : 628\n",
            "POS TP : 629\n",
            "NEG FP : 490\n",
            "NEG FP : 491\n",
            "POS TP : 630\n",
            "POS TP : 631\n",
            "NEG FP : 492\n",
            "POS TP : 632\n",
            "POS TP : 633\n",
            "NEG FP : 493\n",
            "POS TP : 634\n",
            "NEG FP : 494\n",
            "NEG FP : 495\n",
            "POS TP : 635\n",
            "POS FP : 28\n",
            "POS TP : 636\n",
            "POS TP : 637\n",
            "POS TP : 638\n",
            "POS TP : 639\n",
            "NEG FP : 496\n",
            "POS TP : 640\n",
            "NEG FP : 497\n",
            "NEG FP : 498\n",
            "NEG FP : 499\n",
            "POS TP : 641\n",
            "POS TP : 642\n",
            "POS TP : 643\n",
            "NEG FP : 500\n",
            "POS TP : 644\n",
            "POS TP : 645\n",
            "POS TP : 646\n",
            "POS TP : 647\n",
            "POS TP : 648\n",
            "POS TP : 649\n",
            "POS TP : 650\n",
            "POS TP : 651\n",
            "NEG FP : 501\n",
            "POS TP : 652\n",
            "NEG FP : 502\n",
            "NEG FP : 503\n",
            "POS TP : 653\n",
            "NEG TP : 78\n",
            "NEG FP : 504\n",
            "POS TP : 654\n",
            "NEG FP : 505\n",
            "POS TP : 655\n",
            "POS TP : 656\n",
            "POS TP : 657\n",
            "NEG FP : 506\n",
            "POS TP : 658\n",
            "POS TP : 659\n",
            "POS TP : 660\n",
            "POS TP : 661\n",
            "POS TP : 662\n",
            "POS TP : 663\n",
            "POS TP : 664\n",
            "POS TP : 665\n",
            "POS TP : 666\n",
            "POS TP : 667\n",
            "POS TP : 668\n",
            "NEG FP : 507\n",
            "NEG FP : 508\n",
            "POS TP : 669\n",
            "NEG FP : 509\n",
            "NEG FP : 510\n",
            "POS FP : 29\n",
            "POS TP : 670\n",
            "POS TP : 671\n",
            "POS TP : 672\n",
            "POS TP : 673\n",
            "NEG FP : 511\n",
            "POS TP : 674\n",
            "POS TP : 675\n",
            "POS TP : 676\n",
            "POS TP : 677\n",
            "POS TP : 678\n",
            "POS TP : 679\n",
            "POS TP : 680\n",
            "NEG TP : 79\n",
            "NEG FP : 512\n",
            "POS TP : 681\n",
            "POS TP : 682\n",
            "NEG FP : 513\n",
            "POS TP : 683\n",
            "NEG FP : 514\n",
            "POS TP : 684\n",
            "POS TP : 685\n",
            "POS TP : 686\n",
            "NEG FP : 515\n",
            "POS TP : 687\n",
            "POS TP : 688\n",
            "POS TP : 689\n",
            "POS TP : 690\n",
            "POS TP : 691\n",
            "POS TP : 692\n",
            "NEG FP : 516\n",
            "NEG FP : 517\n",
            "POS TP : 693\n",
            "POS TP : 694\n",
            "NEG TP : 80\n",
            "POS TP : 695\n",
            "POS TP : 696\n",
            "NEG FP : 518\n",
            "POS FP : 30\n",
            "POS TP : 697\n",
            "POS TP : 698\n",
            "NEG FP : 519\n",
            "POS TP : 699\n",
            "POS TP : 700\n",
            "NEG FP : 520\n",
            "POS FP : 31\n",
            "NEG FP : 521\n",
            "POS TP : 701\n",
            "NEG FP : 522\n",
            "POS TP : 702\n",
            "POS TP : 703\n",
            "POS TP : 704\n",
            "POS TP : 705\n",
            "POS TP : 706\n",
            "NEG FP : 523\n",
            "NEG TP : 81\n",
            "POS TP : 707\n",
            "POS TP : 708\n",
            "POS TP : 709\n",
            "POS TP : 710\n",
            "NEG TP : 82\n",
            "POS TP : 711\n",
            "POS TP : 712\n",
            "NEG FP : 524\n",
            "POS TP : 713\n",
            "NEG FP : 525\n",
            "POS TP : 714\n",
            "POS TP : 715\n",
            "POS TP : 716\n",
            "POS TP : 717\n",
            "POS TP : 718\n",
            "POS TP : 719\n",
            "NEG FP : 526\n",
            "NEG FP : 527\n",
            "POS TP : 720\n",
            "NEG FP : 528\n",
            "NEG FP : 529\n",
            "NEG FP : 530\n",
            "NEG FP : 531\n",
            "POS TP : 721\n",
            "NEG FP : 532\n",
            "POS TP : 722\n",
            "POS TP : 723\n",
            "NEG FP : 533\n",
            "POS TP : 724\n",
            "NEG FP : 534\n",
            "POS TP : 725\n",
            "POS TP : 726\n",
            "POS TP : 727\n",
            "POS TP : 728\n",
            "POS TP : 729\n",
            "POS TP : 730\n",
            "POS TP : 731\n",
            "POS TP : 732\n",
            "POS TP : 733\n",
            "NEG FP : 535\n",
            "NEG FP : 536\n",
            "POS TP : 734\n",
            "NEG FP : 537\n",
            "POS TP : 735\n",
            "POS TP : 736\n",
            "POS TP : 737\n",
            "POS FP : 32\n",
            "POS FP : 33\n",
            "POS FP : 34\n",
            "POS TP : 738\n",
            "POS TP : 739\n",
            "NEG FP : 538\n",
            "POS TP : 740\n",
            "POS TP : 741\n",
            "POS TP : 742\n",
            "NEG TP : 83\n",
            "POS TP : 743\n",
            "NEG FP : 539\n",
            "NEG FP : 540\n",
            "NEG FP : 541\n",
            "POS TP : 744\n",
            "POS FP : 35\n",
            "POS TP : 745\n",
            "POS TP : 746\n",
            "POS TP : 747\n",
            "POS TP : 748\n",
            "NEG FP : 542\n",
            "POS TP : 749\n",
            "POS TP : 750\n",
            "NEG FP : 543\n",
            "NEG FP : 544\n",
            "NEG FP : 545\n",
            "NEG TP : 84\n",
            "NEG TP : 85\n",
            "NEG FP : 546\n",
            "NEG TP : 86\n",
            "POS TP : 751\n",
            "POS TP : 752\n",
            "NEG FP : 547\n",
            "POS FP : 36\n",
            "NEG FP : 548\n",
            "NEG FP : 549\n",
            "POS TP : 753\n",
            "POS TP : 754\n",
            "POS TP : 755\n",
            "POS TP : 756\n",
            "NEG FP : 550\n",
            "NEG TP : 87\n",
            "NEG FP : 551\n",
            "POS TP : 757\n",
            "NEG TP : 88\n",
            "POS TP : 758\n",
            "NEG TP : 89\n",
            "POS TP : 759\n",
            "NEG FP : 552\n",
            "POS TP : 760\n",
            "NEG TP : 90\n",
            "NEG FP : 553\n",
            "NEG FP : 554\n",
            "POS TP : 761\n",
            "NEG FP : 555\n",
            "POS TP : 762\n",
            "NEG FP : 556\n",
            "POS TP : 763\n",
            "POS TP : 764\n",
            "POS FP : 37\n",
            "POS TP : 765\n",
            "NEG FP : 557\n",
            "POS TP : 766\n",
            "NEG TP : 91\n",
            "NEG FP : 558\n",
            "POS TP : 767\n",
            "POS TP : 768\n",
            "NEG FP : 559\n",
            "POS TP : 769\n",
            "POS TP : 770\n",
            "POS TP : 771\n",
            "POS TP : 772\n",
            "NEG FP : 560\n",
            "POS FP : 38\n",
            "NEG FP : 561\n",
            "POS TP : 773\n",
            "POS TP : 774\n",
            "NEG FP : 562\n",
            "POS TP : 775\n",
            "POS TP : 776\n",
            "NEG FP : 563\n",
            "POS TP : 777\n",
            "NEG FP : 564\n",
            "NEG FP : 565\n",
            "POS TP : 778\n",
            "POS TP : 779\n",
            "POS TP : 780\n",
            "POS TP : 781\n",
            "POS TP : 782\n",
            "POS TP : 783\n",
            "NEG FP : 566\n",
            "NEG FP : 567\n",
            "POS TP : 784\n",
            "NEG FP : 568\n",
            "POS TP : 785\n",
            "POS TP : 786\n",
            "POS TP : 787\n",
            "NEG TP : 92\n",
            "POS TP : 788\n",
            "POS FP : 39\n",
            "POS TP : 789\n",
            "NEG FP : 569\n",
            "NEG FP : 570\n",
            "POS TP : 790\n",
            "POS TP : 791\n",
            "POS TP : 792\n",
            "NEG FP : 571\n",
            "POS TP : 793\n",
            "NEG FP : 572\n",
            "POS TP : 794\n",
            "NEG FP : 573\n",
            "NEG TP : 93\n",
            "POS TP : 795\n",
            "POS TP : 796\n",
            "POS TP : 797\n",
            "POS TP : 798\n",
            "NEG FP : 574\n",
            "POS TP : 799\n",
            "POS TP : 800\n",
            "POS TP : 801\n",
            "POS TP : 802\n",
            "NEG TP : 94\n",
            "POS TP : 803\n",
            "NEG FP : 575\n",
            "NEG TP : 95\n",
            "POS TP : 804\n",
            "POS TP : 805\n",
            "POS TP : 806\n",
            "NEG TP : 96\n",
            "POS TP : 807\n",
            "NEG FP : 576\n",
            "NEG FP : 577\n",
            "POS TP : 808\n",
            "NEG FP : 578\n",
            "NEG FP : 579\n",
            "NEG FP : 580\n",
            "POS TP : 809\n",
            "NEG FP : 581\n",
            "POS TP : 810\n",
            "POS TP : 811\n",
            "POS TP : 812\n",
            "NEG TP : 97\n",
            "NEG FP : 582\n",
            "POS FP : 40\n",
            "POS TP : 813\n",
            "NEG FP : 583\n",
            "POS TP : 814\n",
            "NEG FP : 584\n",
            "NEG TP : 98\n",
            "POS TP : 815\n",
            "POS TP : 816\n",
            "POS TP : 817\n",
            "NEG FP : 585\n",
            "POS TP : 818\n",
            "POS TP : 819\n",
            "POS TP : 820\n",
            "POS TP : 821\n",
            "POS FP : 41\n",
            "NEG FP : 586\n",
            "NEG FP : 587\n",
            "POS TP : 822\n",
            "POS TP : 823\n",
            "POS TP : 824\n",
            "NEG FP : 588\n",
            "NEG FP : 589\n",
            "POS TP : 825\n",
            "POS TP : 826\n",
            "POS TP : 827\n",
            "NEG FP : 590\n",
            "POS TP : 828\n",
            "POS TP : 829\n",
            "POS TP : 830\n",
            "NEG TP : 99\n",
            "NEG FP : 591\n",
            "POS TP : 831\n",
            "POS TP : 832\n",
            "POS TP : 833\n",
            "POS TP : 834\n",
            "POS FP : 42\n",
            "NEG FP : 592\n",
            "POS TP : 835\n",
            "POS TP : 836\n",
            "POS TP : 837\n",
            "POS TP : 838\n",
            "POS TP : 839\n",
            "NEG FP : 593\n",
            "POS FP : 43\n",
            "POS TP : 840\n",
            "POS TP : 841\n",
            "POS TP : 842\n",
            "POS TP : 843\n",
            "NEG FP : 594\n",
            "POS TP : 844\n",
            "POS TP : 845\n",
            "POS TP : 846\n",
            "POS TP : 847\n",
            "POS TP : 848\n",
            "POS TP : 849\n",
            "NEG TP : 100\n",
            "POS TP : 850\n",
            "NEG FP : 595\n",
            "POS TP : 851\n",
            "POS TP : 852\n",
            "POS TP : 853\n",
            "POS TP : 854\n",
            "POS TP : 855\n",
            "POS TP : 856\n",
            "NEG FP : 596\n",
            "NEG FP : 597\n",
            "NEG FP : 598\n",
            "POS FP : 44\n",
            "NEG FP : 599\n",
            "POS TP : 857\n",
            "NEG FP : 600\n",
            "POS TP : 858\n",
            "NEG FP : 601\n",
            "POS TP : 859\n",
            "POS TP : 860\n",
            "NEG FP : 602\n",
            "POS TP : 861\n",
            "NEG TP : 101\n",
            "POS TP : 862\n",
            "POS TP : 863\n",
            "POS TP : 864\n",
            "POS TP : 865\n",
            "NEG FP : 603\n",
            "POS TP : 866\n",
            "POS TP : 867\n",
            "POS TP : 868\n",
            "NEG FP : 604\n",
            "POS TP : 869\n",
            "NEG FP : 605\n",
            "NEG FP : 606\n",
            "POS TP : 870\n",
            "POS TP : 871\n",
            "POS TP : 872\n",
            "NEG FP : 607\n",
            "POS TP : 873\n",
            "POS TP : 874\n",
            "POS TP : 875\n",
            "POS TP : 876\n",
            "NEG FP : 608\n",
            "POS TP : 877\n",
            "POS TP : 878\n",
            "POS TP : 879\n",
            "POS TP : 880\n",
            "POS TP : 881\n",
            "NEG FP : 609\n",
            "POS TP : 882\n",
            "POS TP : 883\n",
            "POS TP : 884\n",
            "POS TP : 885\n",
            "POS TP : 886\n",
            "POS TP : 887\n",
            "POS TP : 888\n",
            "POS TP : 889\n",
            "POS TP : 890\n",
            "POS TP : 891\n",
            "NEG FP : 610\n",
            "POS TP : 892\n",
            "POS TP : 893\n",
            "NEG FP : 611\n",
            "NEG FP : 612\n",
            "POS TP : 894\n",
            "POS TP : 895\n",
            "NEG FP : 613\n",
            "NEG FP : 614\n",
            "NEG FP : 615\n",
            "NEG FP : 616\n",
            "NEG TP : 102\n",
            "NEG FP : 617\n",
            "POS TP : 896\n",
            "NEG FP : 618\n",
            "NEG FP : 619\n",
            "POS TP : 897\n",
            "POS TP : 898\n",
            "NEG FP : 620\n",
            "NEG TP : 103\n",
            "NEG FP : 621\n",
            "POS TP : 899\n",
            "NEG FP : 622\n",
            "POS TP : 900\n",
            "NEG FP : 623\n",
            "POS TP : 901\n",
            "POS TP : 902\n",
            "POS TP : 903\n",
            "POS TP : 904\n",
            "POS FP : 45\n",
            "POS TP : 905\n",
            "POS TP : 906\n",
            "POS TP : 907\n",
            "POS TP : 908\n",
            "POS TP : 909\n",
            "POS TP : 910\n",
            "POS TP : 911\n",
            "POS TP : 912\n",
            "POS TP : 913\n",
            "NEG FP : 624\n",
            "POS TP : 914\n",
            "POS TP : 915\n",
            "POS TP : 916\n",
            "POS TP : 917\n",
            "POS TP : 918\n",
            "NEG FP : 625\n",
            "NEG TP : 104\n",
            "POS TP : 919\n",
            "POS TP : 920\n",
            "POS TP : 921\n",
            "POS TP : 922\n",
            "NEG FP : 626\n",
            "POS TP : 923\n",
            "POS TP : 924\n",
            "NEG FP : 627\n",
            "NEG FP : 628\n",
            "POS TP : 925\n",
            "POS TP : 926\n",
            "POS TP : 927\n",
            "NEG FP : 629\n",
            "POS TP : 928\n",
            "NEG FP : 630\n",
            "POS TP : 929\n",
            "POS TP : 930\n",
            "NEG FP : 631\n",
            "POS TP : 931\n",
            "POS TP : 932\n",
            "POS TP : 933\n",
            "POS TP : 934\n",
            "NEG FP : 632\n",
            "POS TP : 935\n",
            "POS TP : 936\n",
            "POS TP : 937\n",
            "POS TP : 938\n",
            "NEG FP : 633\n",
            "POS TP : 939\n",
            "NEG FP : 634\n",
            "POS FP : 46\n",
            "NEG FP : 635\n",
            "POS TP : 940\n",
            "NEG FP : 636\n",
            "POS TP : 941\n",
            "POS TP : 942\n",
            "NEG FP : 637\n",
            "NEG FP : 638\n",
            "NEG FP : 639\n",
            "POS TP : 943\n",
            "NEG FP : 640\n",
            "NEG FP : 641\n",
            "POS TP : 944\n",
            "NEG FP : 642\n",
            "NEG FP : 643\n",
            "POS TP : 945\n",
            "NEG FP : 644\n",
            "POS TP : 946\n",
            "POS TP : 947\n",
            "POS TP : 948\n",
            "NEG FP : 645\n",
            "NEG TP : 105\n",
            "POS FP : 47\n",
            "POS TP : 949\n",
            "POS TP : 950\n",
            "POS TP : 951\n",
            "NEG FP : 646\n",
            "POS TP : 952\n",
            "NEG FP : 647\n",
            "POS TP : 953\n",
            "POS TP : 954\n",
            "POS TP : 955\n",
            "POS TP : 956\n",
            "NEG FP : 648\n",
            "POS TP : 957\n",
            "POS FP : 48\n",
            "NEG FP : 649\n",
            "POS TP : 958\n",
            "NEG FP : 650\n",
            "POS TP : 959\n",
            "POS TP : 960\n",
            "NEG FP : 651\n",
            "NEG FP : 652\n",
            "POS TP : 961\n",
            "NEG FP : 653\n",
            "POS TP : 962\n",
            "NEG FP : 654\n",
            "POS TP : 963\n",
            "NEG FP : 655\n",
            "NEG FP : 656\n",
            "POS FP : 49\n",
            "NEG FP : 657\n",
            "NEG FP : 658\n",
            "POS TP : 964\n",
            "POS TP : 965\n",
            "NEG TP : 106\n",
            "NEG TP : 107\n",
            "POS TP : 966\n",
            "NEG FP : 659\n",
            "NEG FP : 660\n",
            "NEG FP : 661\n",
            "POS TP : 967\n",
            "NEG TP : 108\n",
            "NEG FP : 662\n",
            "NEG TP : 109\n",
            "POS TP : 968\n",
            "POS TP : 969\n",
            "POS TP : 970\n",
            "POS TP : 971\n",
            "POS TP : 972\n",
            "NEG FP : 663\n",
            "NEG FP : 664\n",
            "NEG FP : 665\n",
            "POS TP : 973\n",
            "POS TP : 974\n",
            "POS TP : 975\n",
            "POS TP : 976\n",
            "NEG TP : 110\n",
            "POS TP : 977\n",
            "POS TP : 978\n",
            "POS TP : 979\n",
            "POS TP : 980\n",
            "POS TP : 981\n",
            "POS TP : 982\n",
            "POS TP : 983\n",
            "POS TP : 984\n",
            "POS TP : 985\n",
            "NEG FP : 666\n",
            "NEG FP : 667\n",
            "POS TP : 986\n",
            "NEG FP : 668\n",
            "POS TP : 987\n",
            "NEG FP : 669\n",
            "NEG FP : 670\n",
            "NEG FP : 671\n",
            "NEG FP : 672\n",
            "NEG FP : 673\n",
            "POS TP : 988\n",
            "NEG FP : 674\n",
            "POS TP : 989\n",
            "POS TP : 990\n",
            "NEG FP : 675\n",
            "NEG FP : 676\n",
            "NEG TP : 111\n",
            "POS TP : 991\n",
            "POS TP : 992\n",
            "POS TP : 993\n",
            "POS TP : 994\n",
            "NEG FP : 677\n",
            "NEG FP : 678\n",
            "POS TP : 995\n",
            "POS TP : 996\n",
            "NEG FP : 679\n",
            "POS TP : 997\n",
            "NEG FP : 680\n",
            "POS TP : 998\n",
            "POS TP : 999\n",
            "POS TP : 1000\n",
            "POS TP : 1001\n",
            "POS TP : 1002\n",
            "POS TP : 1003\n",
            "POS TP : 1004\n",
            "NEG FP : 681\n",
            "POS TP : 1005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-6cbb13f5d8ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mj_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m'reviewText'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mj_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviewText'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/realworldnlp/realworldnlp/predictors.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mpredict_json\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json_to_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjson_to_labeled_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mpredict_instance\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mforward_on_instance\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensors\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minto\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mremove\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     def forward_on_instances(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mforward_on_instances\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0minstance_separated_output\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9e057cd0e121>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, label)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mencoder_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/allennlp/modules/seq2vec_encoders/pytorch_seq2vec_wrapper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, mask, hidden_state)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestoration_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_and_run_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Deal with the fact the LSTM state is a tuple of (state, memory).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/allennlp/modules/encoder_base.py\u001b[0m in \u001b[0;36msort_and_run_forward\u001b[0;34m(self, module, inputs, mask, hidden_state)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# First count how many sequences are empty.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mnum_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0msequence_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lengths_from_binary_sequence_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 1 with size 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t86K47iOvl5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_tp_file = open('../categoryName_pos_tp.txt', 'w+')\n",
        "pos_fp_file = open('../categoryName_pos_fp.txt', 'w+')\n",
        "neg_tp_file = open('../categoryName_neg_tp.txt', 'w+')\n",
        "neg_fp_file = open('../categoryName_neg_fp.txt', 'w+')\n",
        "for line in pos_tp:\n",
        "  pos_tp_file.write(line)\n",
        "for line in pos_fp:\n",
        "  pos_fp_file.write(line)\n",
        "for line in neg_tp:\n",
        "  neg_tp_file.write(line)\n",
        "for line in neg_fp:\n",
        "  neg_fp_file.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0X2PY7iPpId",
        "colab_type": "text"
      },
      "source": [
        "## Task 5 \n",
        "\n",
        "Calculate approximate precision values based on your mapping from task 4.\n",
        "\n",
        "Store the calculations as part of a readme or send the values by e-mail submission. \n",
        "\n",
        "Submit the files from Task 3 and Task 4 as your group submission.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGtRCQWPpIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}