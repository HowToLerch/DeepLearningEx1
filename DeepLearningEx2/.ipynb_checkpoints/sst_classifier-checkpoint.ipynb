{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T1lqbTTNPpFB"
   },
   "source": [
    "# Sentiment Classification with a Deep Learning Model\n",
    "\n",
    "This notebook introduces a machine learning task from the field of natural language processing (machine learning focused on the processing of spoken and written text).\n",
    "\n",
    "## Sentiment Analysis\n",
    "\n",
    "The modelled task is a classification task called sentiment analysis. \n",
    "Text snippets are classified according to their positive or negative sentiment that is expressed in them. \n",
    "This can be modelled as 3-class problem (negative, neutral, positive), or as a degree of sentiment on a 5-class or 10-class scale. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Acknowledgement\n",
    "\n",
    "The notebook is based on https://www.manning.com/books/real-world-natural-language-processing, an upcoming book focused on NLP.\n",
    "\n",
    "The ML frameworks used are:\n",
    "\n",
    "* pytorch\n",
    "* allennlp\n",
    "* spacy\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/erikgraf/deepLearning/blob/master/Deep_Learning_Sentiment_classifier.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0CbtvBLYPpFJ"
   },
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "The cell below installs the main dependencies and clones some a repository that forms the basis of the implementation. \n",
    "\n",
    "Executing it with `CTRL + Enter` (`STRG +Enter` on a german keyboard) could take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2WUYWOVTcajS",
    "outputId": "816531a3-e8bf-4038-cc00-b9eb9d26064c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting allennlp\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
      "Requirement already satisfied: requests>=2.18 in d:\\anaconda3\\lib\\site-packages (from allennlp) (2.22.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda3\\lib\\site-packages (from allennlp) (1.16.5)\n",
      "Collecting flaky (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
      "Collecting tensorboardX>=1.2 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
      "Requirement already satisfied: h5py in d:\\anaconda3\\lib\\site-packages (from allennlp) (2.9.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda3\\lib\\site-packages (from allennlp) (0.21.3)\n",
      "Collecting pytorch-transformers==1.1.0 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
      "Requirement already satisfied: pytest in d:\\anaconda3\\lib\\site-packages (from allennlp) (5.2.1)\n",
      "Collecting pytorch-pretrained-bert>=0.6.0 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
      "Requirement already satisfied: tqdm>=4.19 in d:\\anaconda3\\lib\\site-packages (from allennlp) (4.36.1)\n",
      "Requirement already satisfied: flask>=1.0.2 in d:\\anaconda3\\lib\\site-packages (from allennlp) (1.1.1)\n",
      "Collecting word2number>=1.1 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
      "Collecting flask-cors>=3.0.7 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
      "Collecting boto3 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/00/34/75b2d38f0647cfbdfd00c62c1d3e4210f6c40fb8ff66a9a644c439e849ab/boto3-1.11.1-py2.py3-none-any.whl (128kB)\n",
      "Collecting overrides (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/72/dd/ac49f9c69540d7e09210415801a05d0a54d4d0ca8401503c46847dacd3a0/overrides-2.8.0.tar.gz\n",
      "Collecting editdistance (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/09/c0/e3ee9a2344d333c1b6e66022297dad3ac20199e092f168bd53e8af966d29/editdistance-0.5.3-cp36-cp36m-win_amd64.whl\n",
      "Collecting parsimonious>=0.8.0 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in d:\\anaconda3\\lib\\site-packages (from allennlp) (3.1.1)\n",
      "Collecting ftfy (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\anaconda3\\lib\\site-packages (from allennlp) (2019.3)\n",
      "Collecting conllu==1.3.1 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
      "Collecting jsonpickle (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
      "Collecting torch>=1.2.0 (from allennlp)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Could not find a version that satisfies the requirement torch>=1.2.0 (from allennlp) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)\n",
      "ERROR: No matching distribution found for torch>=1.2.0 (from allennlp)\n"
     ]
    }
   ],
   "source": [
    "!pip install allennlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "ni4FWBoJPpFl",
    "outputId": "7ff3f05a-63dc-49a0-8561-a81696382b99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicolas\\Documents\\GitHub\\DeepLearningEx1\\DeepLearningEx2\\realworldnlp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'realworldnlp'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mhagiwara/realworldnlp.git\n",
    "%cd realworldnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIoxbXqOPpFx"
   },
   "source": [
    "## Imports\n",
    "\n",
    "Execute the cell below to load all required modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FM5pBpj7cajc"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-911a9bfae80d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_readers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstanford_sentiment_tree_bank\u001b[0m \u001b[1;32mimport\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from allennlp.data.dataset_readers.stanford_sentiment_tree_bank import \\\n",
    "    StanfordSentimentTreeBankDatasetReader\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "from realworldnlp.predictors import SentenceClassifierPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6LmBTaVlPpGC"
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "The cell below sets the hyperparameters.\n",
    "\n",
    "* EMBEDDING_DIM: This is the dimensionality of the word embeddings (numeric representations of words such as word2vec or glove (https://nlp.stanford.edu/projects/glove/))\n",
    "* HIDDEN_DIM: This is the dimensionality of the LSTM (Long Short Term Memory) Deep Learning network. \n",
    "\n",
    "A value of 128 is pretty standard for the embeddings and hidden_dim.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jecKS_nhcajq"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EzH3khPHPpGN"
   },
   "source": [
    "## Training Data Set\n",
    "\n",
    "For training we will use the Stanford Sentiment Treebank data set.\n",
    "A data set for training sentiment analysis models. It is annotated both on the sentence and the word level with regard to the sentiment. \n",
    "\n",
    "When loading the data set we can configure the granularity to `'5-class'` or `'3-class'`.\n",
    "\n",
    "`'3-class'` represents classification on the level of `negative`, `neutral`, `positive` encoded as `0`, `1`, `2` (positive). `'5-class'` on a level from `0` to `4`.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-yyKUYxcaj1"
   },
   "outputs": [],
   "source": [
    "reader = StanfordSentimentTreeBankDatasetReader(granularity='5-class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "N4PBhQZAcaj6",
    "outputId": "74a42c3f-73e6-44ff-a9bc-3fc346f21100"
   },
   "outputs": [],
   "source": [
    "train_dataset = reader.read('https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/train.txt')\n",
    "dev_dataset = reader.read('https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1c2Rh7tPpGg"
   },
   "source": [
    "## Model Implementation in AllenNLP\n",
    "\n",
    "Execute the cell below to load the model classification.\n",
    "\n",
    "Depending on the class level chosen (3 vs 5) change the positive label in the init method to ('2' or '4').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5V2kzVAlcajw"
   },
   "outputs": [],
   "source": [
    "# Model in AllenNLP represents a model that is trained.\n",
    "@Model.register(\"lstm_classifier\")\n",
    "class LstmClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 vocab: Vocabulary,\n",
    "                 positive_label: str = '4') -> None:\n",
    "        super().__init__(vocab)\n",
    "        # We need the embeddings to convert word IDs to their vector representations\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # After converting a sequence of vectors to a single vector, we feed it into\n",
    "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
    "        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                      out_features=vocab.get_vocab_size('labels'))\n",
    "\n",
    "        # Monitor the metrics - we use accuracy, as well as prec, rec, f1 for 4 (very positive)\n",
    "        positive_index = vocab.get_token_index(positive_label, namespace='labels')\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        self.f1_measure = F1Measure(positive_index)\n",
    "\n",
    "        # We use the cross entropy loss because this is a classification task.\n",
    "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
    "        # which makes it unnecessary to add a separate softmax layer.\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Instances are fed to forward after batching.\n",
    "    # Fields are passed through arguments with the same name.\n",
    "    def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
    "        # shorter sequences get padded with zeros to make them equal length.\n",
    "        # Masking is the process to ignore extra zeros added by padding\n",
    "        mask = get_text_field_mask(tokens)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.linear(encoder_out)\n",
    "\n",
    "        # In AllenNLP, the output of forward() is a dictionary.\n",
    "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            self.f1_measure(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        precision, recall, f1_measure = self.f1_measure.get_metric(reset)\n",
    "        return {'accuracy': self.accuracy.get_metric(reset),\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_measure': f1_measure}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89KdsW5kPpGq"
   },
   "source": [
    "## Transform Text into Numeric Representation\n",
    "\n",
    "The following cells are responsible for the transformation of text in string form into numeric representations that are suitable as learning input for the neural network.\n",
    "\n",
    "1. Extract vocabulary of unique terms from the text\n",
    "2. Create embeddings for the terms\n",
    "3. Define transformation (encoding) for a sequence of text (i.e. a sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OuchQwj5cakA",
    "outputId": "ab0a66ca-e609-4905-a55a-c46bda5ae568"
   },
   "outputs": [],
   "source": [
    "# You can optionally specify the minimum count of tokens/labels.\n",
    "# `min_count={'tokens':3}` here means that any tokens that appear less than three times\n",
    "# will be ignored and not included in the vocabulary.\n",
    "vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
    "                                  min_count={'tokens': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDx14NvHcakC"
   },
   "outputs": [],
   "source": [
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VKXSf0wucakG"
   },
   "outputs": [],
   "source": [
    "# BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,\n",
    "# not for labels, which are used as-is as the \"answer\" of the sentence classification\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "93pvAweOcakM"
   },
   "outputs": [],
   "source": [
    "# Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
    "# (usually a sequence of embedded word vectors), processes it, and returns a single\n",
    "# vector. Oftentimes this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
    "# AllenNLP also supports CNNs and other simple architectures (for example,\n",
    "# just averaging over the input vectors).\n",
    "encoder = PytorchSeq2VecWrapper(\n",
    "    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4atXjVUsPpHI"
   },
   "source": [
    "## Configure Model for Training\n",
    "\n",
    "The following four cells configure the model for training.\n",
    "\n",
    "1. The LstmClassifier class takes the word_embeddings, the define sequence encoder and the vocabulary as input configuration. \n",
    "\n",
    "2. The BucketIterator is a helper class for iterating over the full training set and randomly selects batches of instances for the training. \n",
    "\n",
    "3. optimizer specifies the learning rate for Adam (a mathmatical optimisation function that will guide the weight adaptations of our model).\n",
    "\n",
    "4. trainer holds our instatiation of the model, and defines the number of epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZuuP66iccakR"
   },
   "outputs": [],
   "source": [
    "model = LstmClassifier(word_embeddings, encoder, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ip0BO9QecakY"
   },
   "outputs": [],
   "source": [
    "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ccuqvd6rcakg"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uu_dBwd1cakk"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=dev_dataset,\n",
    "                  patience=40,\n",
    "                  num_epochs=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cDt-Kg1oPpHo"
   },
   "source": [
    "## Train\n",
    "\n",
    "Execute the cell below to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "55xc9RfYPpHp",
    "outputId": "bd8591c0-e4f4-423c-bdd8-9c9213fe275d"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFacAYSlPpHw"
   },
   "source": [
    "## Sanity Check\n",
    "\n",
    "The cell below will allow you to enter sample sentences and test the predictions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oxRELM_-cako",
    "outputId": "0860cabb-cb9a-4cf5-9e11-65fc924c7433"
   },
   "outputs": [],
   "source": [
    "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
    "logits = predictor.predict(\"Don't waste your money\")['logits']\n",
    "label_id = np.argmax(logits)\n",
    "\n",
    "print(model.vocab.get_token_from_index(label_id, 'labels'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wiZLRRYPpIJ"
   },
   "source": [
    "## More Substantive Checks\n",
    "\n",
    "In order to do some more in depth checks how well the model does, and how well it might generalize we can utilize a set of Amazon reviews. \n",
    "\n",
    "http://jmcauley.ucsd.edu/data/amazon/\n",
    "\n",
    "The site above holds a very large of Amazon reviews that can be used for scientific purposes. \n",
    "\n",
    "### Task 1: Choose and Download a Subcategory\n",
    "\n",
    "From the table below, choose a category that you will use for testing. \n",
    "Download the 5 core links that hold the full text, title and rating of a review. \n",
    "\n",
    "\n",
    "<html>\n",
    "\n",
    "<table>\n",
    "<tbody><tr>\n",
    "  <td>Books</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_10.json.gz\">10-core</a> (4,701,968 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz\">5-core</a> (8,898,041 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Books.csv\">ratings only</a> (22,507,155 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Electronics</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_10.json.gz\">10-core</a> (347,393 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\">5-core</a> (1,689,188 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Electronics.csv\">ratings only</a> (7,824,482 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Movies and TV</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Movies_and_TV_10.json.gz\">10-core</a> (958,986 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Movies_and_TV_5.json.gz\">5-core</a> (1,697,533 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Movies_and_TV.csv\">ratings only</a> (4,607,047 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>CDs and Vinyl</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_CDs_and_Vinyl_10.json.gz\">10-core</a> (445,412 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_CDs_and_Vinyl_5.json.gz\">5-core</a> (1,097,592 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_CDs_and_Vinyl.csv\">ratings only</a> (3,749,004 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Clothing, Shoes and Jewelry</td>\n",
    "  <!-- <td></td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Clothing_Shoes_and_Jewelry_5.json.gz\">5-core</a> (278,677 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Clothing_Shoes_and_Jewelry.csv\">ratings only</a> (5,748,920 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Home and Kitchen</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Home_and_Kitchen_10.json.gz\">10-core</a> (25,445 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Home_and_Kitchen_5.json.gz\">5-core</a> (551,682 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Home_and_Kitchen.csv\">ratings only</a> (4,253,926 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Kindle Store</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Kindle_Store_10.json.gz\">10-core</a> (367,478 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Kindle_Store_5.json.gz\">5-core</a> (982,619 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Kindle_Store.csv\">ratings only</a> (3,205,467 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Sports and Outdoors</td>\n",
    "  <!-- <td></td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Sports_and_Outdoors_5.json.gz\">5-core</a> (296,337 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Sports_and_Outdoors.csv\">ratings only</a> (3,268,695 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Cell Phones and Accessories</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_10.json.gz\">10-core</a> (1,854 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz\">5-core</a> (194,439 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Cell_Phones_and_Accessories.csv\">ratings only</a> (3,447,249 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Health and Personal Care</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Health_and_Personal_Care_10.json.gz\">10-core</a> (55,076 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Health_and_Personal_Care_5.json.gz\">5-core</a> (346,355 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Health_and_Personal_Care.csv\">ratings only</a> (2,982,326 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Toys and Games</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Toys_and_Games_10.json.gz\">10-core</a> (18,637 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Toys_and_Games_5.json.gz\">5-core</a> (167,597 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Toys_and_Games.csv\">ratings only</a> (2,252,771 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Video Games</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Video_Games_10.json.gz\">10-core</a> (52,158 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Video_Games_5.json.gz\">5-core</a> (231,780 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Video_Games.csv\">ratings only</a> (1,324,753 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Tools and Home Improvement</td>\n",
    "  <!-- <td></td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Tools_and_Home_Improvement_5.json.gz\">5-core</a> (134,476 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Tools_and_Home_Improvement.csv\">ratings only</a> (1,926,047 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Beauty</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Beauty_10.json.gz\">10-core</a> (28,798 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Beauty_5.json.gz\">5-core</a> (198,502 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Beauty.csv\">ratings only</a> (2,023,070 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Apps for Android</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Apps_for_Android_10.json.gz\">10-core</a> (264,050 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Apps_for_Android_5.json.gz\">5-core</a> (752,937 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Apps_for_Android.csv\">ratings only</a> (2,638,172 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Office Products</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Office_Products_10.json.gz\">10-core</a> (25,374 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Office_Products_5.json.gz\">5-core</a> (53,258 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Office_Products.csv\">ratings only</a> (1,243,186 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Pet Supplies</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Pet_Supplies_10.json.gz\">10-core</a> (3,152 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Pet_Supplies_5.json.gz\">5-core</a> (157,836 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Pet_Supplies.csv\">ratings only</a> (1,235,316 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Automotive</td>\n",
    "  <!-- <td></td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Automotive_5.json.gz\">5-core</a> (20,473 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Automotive.csv\">ratings only</a> (1,373,768 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Grocery and Gourmet Food</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Grocery_and_Gourmet_Food_10.json.gz\">10-core</a> (37,348 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Grocery_and_Gourmet_Food_5.json.gz\">5-core</a> (151,254 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Grocery_and_Gourmet_Food.csv\">ratings only</a> (1,297,156 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Patio, Lawn and Garden</td>\n",
    "  <!-- <td></td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Patio_Lawn_and_Garden_5.json.gz\">5-core</a> (13,272 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Patio_Lawn_and_Garden.csv\">ratings only</a> (993,490 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Baby</td>\n",
    "  <!-- <td></td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Baby_5.json.gz\">5-core</a> (160,792 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Baby.csv\">ratings only</a> (915,446 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Digital Music</td>\n",
    "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Digital_Music_10.json.gz\">10-core</a> (22,772 reviews)</td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Digital_Music_5.json.gz\">5-core</a> (64,706 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Digital_Music.csv\">ratings only</a> (836,006 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Musical Instruments</td>\n",
    "  <!-- <td></td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz\">5-core</a> (10,261 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Musical_Instruments.csv\">ratings only</a> (500,176 ratings)</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>Amazon Instant Video</td>\n",
    "  <!-- <td></td> -->\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Amazon_Instant_Video_5.json.gz\">5-core</a> (37,126 reviews)</td>\n",
    "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Amazon_Instant_Video.csv\">ratings only</a> (583,933 ratings)</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "    </table>\n",
    "</html>\n",
    "\n",
    "### Task 2\n",
    "\n",
    "Sanity checks with the reviews.\n",
    "\n",
    "The format of the files is as follows:\n",
    "\n",
    "`{\n",
    "\t\"reviewerID\": \"A2ICI6VUC0U5K6\",\n",
    "\t\"asin\": \"B0014JKKGK\",\n",
    "\t\"reviewerName\": \"Jermin Botrous \\\"gigigigi\\\"\",\n",
    "\t\"helpful\": [0, 0],\n",
    "\t\"reviewText\": \"Don't waste your money because elastic goes bad after 2 washes\",\n",
    "\t\"overall\": 1.0,\n",
    "\t\"summary\": \"One Star\",\n",
    "\t\"unixReviewTime\": 1404432000,\n",
    "\t\"reviewTime\": \"07 4, 2014\"\n",
    "}`\n",
    "\n",
    "Use the following code snippets to load individal review texts.\n",
    "\n",
    "Opening a file in python:\n",
    "\n",
    "``\n",
    "test_file = open('file_name.json', 'r')\n",
    "first_line = test_file.readline()\n",
    "``\n",
    "\n",
    "Transform the line into a json object to access the individual fiels (such as reviewText).\n",
    "\n",
    "\n",
    "``\n",
    "import json\n",
    "j_obj = json.loads(first_line)\n",
    "print('reviewText:' + j_obj['reviewText'])\n",
    "``\n",
    "\n",
    "finally use the code from above to test the predictions\n",
    "\n",
    "``\n",
    "logits = predictor.predict(\"Don't waste your money\")['logits']\n",
    "label_id = np.argmax(logits)\n",
    "prediction = model.vocab.get_token_from_index(label_id, 'labels')\n",
    "print(prediction)\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "2qCs1IUoPpIJ",
    "outputId": "946b1e45-0148-4c15-90c6-61811d4e1ab7"
   },
   "outputs": [],
   "source": [
    "test_file = open('../reviews_Digital_Music_5.json', 'r')\n",
    "first_line = test_file.readline()\n",
    "print(first_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "mdzxAtyzb9w0",
    "outputId": "a91239b3-5f44-4a42-9147-8966a4580e0d"
   },
   "outputs": [],
   "source": [
    "import json \n",
    "j_obj = json.loads(first_line) \n",
    "print('reviewText:' + j_obj['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "O4ohLt95caa4",
    "outputId": "60e0851e-ec9f-49b7-a7c8-d950baa74c38"
   },
   "outputs": [],
   "source": [
    "logits = predictor.predict(\"Don't waste your money\")['logits']\n",
    "label_id = np.argmax(logits)\n",
    "prediction = model.vocab.get_token_from_index(label_id, 'labels')\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZjtOtcz0PpIQ"
   },
   "source": [
    "## Task 3\n",
    "\n",
    "Iterate over the reviews and extract:\n",
    "\n",
    "* 100 positive predictions (i.e. 4)\n",
    "* 100 negative predictions (i.e. 0)\n",
    "\n",
    "Save the sets of positive and negative predictions as plain text files:\n",
    "\n",
    "* categoryName_100_pos.txt\n",
    "* categoryName_100_neg.txt\n",
    "\n",
    "Manually inspect the predictions to identify potential false positives in boths sets.\n",
    "Store a couple of those false positives in the files:\n",
    "\n",
    "* categoryName_100_pos_fp.txt\n",
    "* categoryName_100_neg_fp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Z9tJNPo7PpIR",
    "outputId": "1a5ff56d-51cb-4803-edba-97aceab8eb12"
   },
   "outputs": [],
   "source": [
    "test_file = open('../reviews_Digital_Music_5.json', 'r')\n",
    "lines = test_file.readlines()\n",
    "positives = []\n",
    "negatives = []\n",
    "for line in lines:\n",
    "  j_obj = json.loads(line) \n",
    "  logits = predictor.predict(j_obj['reviewText'])['logits']\n",
    "  label_id = np.argmax(logits)\n",
    "  prediction = model.vocab.get_token_from_index(label_id, 'labels')\n",
    "  print(prediction)\n",
    "  if prediction == '4' and len(positives) < 100:\n",
    "    positives.append(line)\n",
    "    print(\"Pos len : \"+ str(len(positives)))\n",
    "  elif prediction == '0' and len(negatives) < 100:\n",
    "    negatives.append(line)\n",
    "    print(\"Neg len : \"+ str(len(negatives)))\n",
    "  if len(positives) >= 100 and len(negatives) >= 100:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "-J6mrvZFhtkM",
    "outputId": "35e676f4-0931-44da-d2c5-5c0751fb6ed3"
   },
   "outputs": [],
   "source": [
    "print(positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "twnh1xiLkVzx"
   },
   "outputs": [],
   "source": [
    "positive_file = open('../categoryName_100_pos.txt', 'w+')\n",
    "for line in positives:\n",
    "  positive_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_EycqV5lCdZ"
   },
   "outputs": [],
   "source": [
    "negative_file = open('../categoryName_100_neg.txt', 'w+')\n",
    "for line in negatives:\n",
    "  negative_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "khfG58m2PpIV"
   },
   "source": [
    "## Task 4\n",
    "\n",
    "Generate listing of false positives.\n",
    "Analyse the data from Amazon.\n",
    "What would be a way to utilize this data in order to generate larger lists of false positives?\n",
    "Derive a method that will allow you to predict over the full content of the file and create lists of:\n",
    "* True positive 'positive' predictions\n",
    "* False positive 'positive' predictions\n",
    "* True positive 'negative' predictions\n",
    "* False positive 'negative' predictions\n",
    "\n",
    "Save the four sets four your group submission:\n",
    "\n",
    "* categoryName_pos_tp.txt\n",
    "* categoryName_pos_fp.txt\n",
    "* categoryName_neg_tp.txt\n",
    "* categoryName_neg_fp.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dDD6E7I2PpIZ",
    "outputId": "f97daaea-d1eb-4607-f3a9-e093335e43f7"
   },
   "outputs": [],
   "source": [
    "test_file = open('../reviews_Digital_Music_5.json', 'r')\n",
    "lines = test_file.readlines()\n",
    "pos_tp = []\n",
    "pos_fp = []\n",
    "neg_tp = []\n",
    "neg_fp = []\n",
    "for line in lines:\n",
    "  try:\n",
    "    j_obj = json.loads(line)\n",
    "    logits = predictor.predict(j_obj['reviewText'])['logits']\n",
    "    label_id = np.argmax(logits)\n",
    "    prediction = model.vocab.get_token_from_index(label_id, 'labels')\n",
    "    if prediction == '4':\n",
    "      if j_obj['overall'] > 3:\n",
    "        pos_tp.append(line)\n",
    "        #print(\"POS TP : \"+ str(len(pos_tp)))\n",
    "      else:\n",
    "        pos_fp.append(line)\n",
    "        #print(\"POS FP : \"+ str(len(pos_fp)))\n",
    "    elif prediction == '0':\n",
    "      if j_obj['overall'] <= 3:\n",
    "        neg_tp.append(line)\n",
    "        #print(\"NEG TP : \"+ str(len(neg_tp)))\n",
    "      else:\n",
    "        neg_fp.append(line)\n",
    "        #print(\"NEG FP : \"+ str(len(neg_fp)))\n",
    "  except:\n",
    "    print(line)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t86K47iOvl5S"
   },
   "outputs": [],
   "source": [
    "pos_tp_file = open('../categoryName_pos_tp.txt', 'w+')\n",
    "pos_fp_file = open('../categoryName_pos_fp.txt', 'w+')\n",
    "neg_tp_file = open('../categoryName_neg_tp.txt', 'w+')\n",
    "neg_fp_file = open('../categoryName_neg_fp.txt', 'w+')\n",
    "for line in pos_tp:\n",
    "  pos_tp_file.write(line)\n",
    "for line in pos_fp:\n",
    "  pos_fp_file.write(line)\n",
    "for line in neg_tp:\n",
    "  neg_tp_file.write(line)\n",
    "for line in neg_fp:\n",
    "  neg_fp_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0X2PY7iPpId"
   },
   "source": [
    "## Task 5 \n",
    "\n",
    "Calculate approximate precision values based on your mapping from task 4.\n",
    "\n",
    "Store the calculations as part of a readme or send the values by e-mail submission. \n",
    "\n",
    "Submit the files from Task 3 and Task 4 as your group submission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "kCGtRCQWPpIe",
    "outputId": "0f44ab84-37b2-4438-8d6b-ba342bd7b37a"
   },
   "outputs": [],
   "source": [
    "#(truepos + trueneg) / total amount = precision\n",
    "\n",
    "pos_tp_file = open('../categoryName_pos_tp.txt', 'r')\n",
    "pos_fp_file = open('../categoryName_pos_fp.txt', 'r')\n",
    "neg_tp_file = open('../categoryName_neg_tp.txt', 'r')\n",
    "neg_fp_file = open('../categoryName_neg_fp.txt', 'r')\n",
    "\n",
    "amount_pos_tp = len(pos_tp_file.readlines())\n",
    "print(amount_pos_tp)\n",
    "\n",
    "amount_pos_fp = len(pos_fp_file.readlines())\n",
    "print(amount_pos_fp)\n",
    "\n",
    "amount_neg_tp = len(neg_tp_file.readlines())\n",
    "print(amount_neg_tp)\n",
    "\n",
    "amount_neg_fp = len(neg_fp_file.readlines())\n",
    "print(amount_neg_fp)\n",
    "\n",
    "amount_truePredicted = 0\n",
    "amount_wrongPredicted = 0\n",
    "\n",
    "amount_truePredicted += amount_pos_tp\n",
    "amount_truePredicted += amount_neg_tp\n",
    "\n",
    "amount_wrongPredicted += amount_pos_fp \n",
    "amount_wrongPredicted += amount_neg_fp\n",
    "total = amount_truePredicted + amount_wrongPredicted\n",
    "\n",
    "precision = amount_truePredicted/total\n",
    "print(precision)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of sst_classifier.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
